{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a58994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1166aab4",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f2a45a",
   "metadata": {},
   "source": [
    "Set the environment and variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c47b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import random\n",
    "import typing\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "import itertools\n",
    "from torch import nn\n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d398373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_addr = \"/nfs/home/aliami/mimic4_dataset/physionet.org/files/mimiciv/2.2/hosp/pre_proc_data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14c95db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open (root_addr+'saved_idx/mimic4_icd9_X_train_3spilit', 'rb') as fp:\n",
    "    X_train_idx_main = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be33b78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9072,\n",
       " 773,\n",
       " ['5723', '78959', '5715', '07070', '496'],\n",
       " ['N02BE01', 'C03CA01', 'J05AX08', 'S01XA14', 'R03CC02'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the unique diags and medications lists\n",
    "uniq_diags = np.load(root_addr+'mimic4_uniq_icd9.npy',allow_pickle='TRUE').tolist()\n",
    "uniq_meds = np.load(root_addr+'mimic4_uniq_atc5.npy',allow_pickle='TRUE').tolist()\n",
    "len(uniq_diags), len(uniq_meds), uniq_diags[:5], uniq_meds[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea652797",
   "metadata": {},
   "source": [
    "Map the Patient trajectories to text format:\n",
    "\n",
    "    medical code -> token\n",
    "    new visit -> dot\n",
    "    visit -> sentence\n",
    "    patient trajectory -> document\n",
    "    \n",
    "output e.g.\n",
    "\n",
    "    m_N02B d_6103 m_A10A . m_A06A d_3485 m_A04 . m_N02A m_V04C\n",
    "    \n",
    "For the interpreteability purposes we added d_ before ICD9 codes and m_ before ATC5 codes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54f3b002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172980\n",
      "121086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 121086/121086 [06:30<00:00, 309.86it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_selected_ind_dict(inp_dict, indxs):\n",
    "    inp_dict = pd.Series(inp_dict) \n",
    "    out_dict = {}\n",
    "    for i in indxs:\n",
    "        out_dict[inp_dict.index[i]]= inp_dict.iloc[i]\n",
    "    return out_dict\n",
    "\n",
    "##load data\n",
    "\n",
    "# Load dictionary\n",
    "pat_id_visist_dict = np.load(root_addr+'mimic4_all_pat_icd9_atc5.npy',allow_pickle='TRUE').item()\n",
    "\n",
    "print(len(pat_id_visist_dict.keys()))\n",
    "pat_id_visist_dict = get_selected_ind_dict(pat_id_visist_dict, X_train_idx_main)\n",
    "print(len(pat_id_visist_dict.keys()))\n",
    "\n",
    "num_visits = 100\n",
    "\n",
    "#convert key-value dataset to visit_seq dataframe\n",
    "def make_historical_dataframe(patient_diagnosis_dict, num_visit_per_patient=num_visits, target_icd9_code= \"428\",\n",
    "                              zero_padding=False,\n",
    "                             codes_proc=True): #428\n",
    "  \n",
    "  data_disease_df_list = []\n",
    "  dtarget = \"1\"\n",
    "  for key, val in tqdm(patient_diagnosis_dict.items()):\n",
    "    #reverse the order of visit to have them to have past:furure order\n",
    "    #because in our preprocessing we put the last visit at the first element of the list\n",
    "    #The mimic dataset is in the right order and does not need to revse its order\n",
    "    \n",
    "    #delete tajectories with only one visit\n",
    "    if len(val)==1:\n",
    "        continue\n",
    "    \n",
    "    #val = list(reversed(val))\n",
    "    #truncate to first 4 digits\n",
    "    visits_seq = []\n",
    "    if codes_proc:\n",
    "        for vis in val:\n",
    "            #visits_seq.append([code[:4] for code in vis])\n",
    "            visits_seq.append([\"d_\"+code[:4] if code in uniq_diags else \"m_\"+code[:4] for code in vis])\n",
    "        val = visits_seq\n",
    "    patient_diagnosis_dict[key] = visits_seq\n",
    "    if len(val) >= num_visit_per_patient:\n",
    "        data_disease_df_list.append([*val[-num_visit_per_patient:], dtarget, key])\n",
    "    else:\n",
    "        if zero_padding:\n",
    "            num_of_zero_cols = -(len(val) - num_visit_per_patient)\n",
    "            temp_list = [[\"empty\"] for i in range(num_of_zero_cols)]\n",
    "            data_disease_df_list.append([*temp_list,*val, dtarget, key])\n",
    "            \n",
    "  col_names = [\"visit\"+str(i) for i in range(1,num_visit_per_patient+1)] + [\"target\"] + [\"patient_id\"]\n",
    "  data_disease_df = pd.DataFrame(data_disease_df_list, columns=col_names)\n",
    "  return data_disease_df, patient_diagnosis_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_disease_df, pat_id_visist_dict_x = make_historical_dataframe(pat_id_visist_dict,\n",
    "                                                                num_visit_per_patient=num_visits,\n",
    "                                                                zero_padding=True)\n",
    "data_disease_df\n",
    "\n",
    "#convert  visit_seq dataframe of visits to sequences of sentences\n",
    "#word=code, visit=sentence\n",
    "def visit_sec_to_sen_seq(vis_seq_df, number_of_visits=num_visits, duplicate_allowed = True):\n",
    "    vis_seq_df = (vis_seq_df.iloc[:,0:number_of_visits])\n",
    "    vis_seq_str_list = []\n",
    "    for index, row in vis_seq_df.iterrows():\n",
    "        #delete empty visits\n",
    "        if not duplicate_allowed:\n",
    "            row = pd.Series([list(set(visit)) for visit in row])\n",
    "        row = list(filter(lambda a: a != [\"empty\"], row.tolist()))\n",
    "        seq_sen = \" . \".join([\" \".join(vis) for vis in row])\n",
    "        vis_seq_str_list.append(seq_sen)\n",
    "    return vis_seq_str_list#pd.DataFrame(vis_seq_str_list).astype(str)\n",
    "\n",
    "\n",
    "vis_seq_df_str = visit_sec_to_sen_seq(data_disease_df, duplicate_allowed=False)\n",
    "#vis_seq_df_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63855ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c79bf93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "vis_seq_df_str = np.load(root_addr+'mimic4_icd9_atc5_long_100vis_seq_df_str.npy',allow_pickle='TRUE').tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49bed0c",
   "metadata": {},
   "source": [
    "Spliting data to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c56de902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47548, 5284)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_idx, X_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(vis_seq_df_str)),\n",
    "                                                    range(len(vis_seq_df_str)),\n",
    "                                                    test_size=0.10,\n",
    "                                                    random_state=999)\n",
    "\n",
    "len(X_train_idx), len(X_test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b9177b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47548, 5284)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make train and test data based on their indexes\n",
    "X_train, X_val = list(np.array(vis_seq_df_str)[X_train_idx]), list(np.array(vis_seq_df_str)[X_test_idx])\n",
    "\n",
    "\n",
    "len(X_train), len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1088622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the length of trajecrories distribution \n",
    "nim_vis_per_pat = []\n",
    "for pat in (vis_seq_df_str):\n",
    "    nim_vis_per_pat.append(len(pat.split(\" \")))\n",
    "    \n",
    "np.quantile(nim_vis_per_pat, .7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "663cdc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Density'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.kdeplot(nim_vis_per_pat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92522faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7203e860",
   "metadata": {},
   "source": [
    "# Building the Conditional Code Swapping (CCS) weighting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c5852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceb9f40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 52832/52832 [00:00<00:00, 257700.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4688"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the unique medical codes\n",
    "list_toks = []\n",
    "for pat in tqdm(vis_seq_df_str):\n",
    "    vis_traj = pat.split(\".\")\n",
    "    for vis in vis_traj:\n",
    "        list_toks.extend(vis.split(\" \"))\n",
    "uniq_toks = list(np.unique(list_toks))\n",
    "len(uniq_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b111c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build an empty 2d numpy matrix\n",
    "transit_mat = np.zeros((len(uniq_toks), len(uniq_toks)),int)\n",
    "\n",
    "def fill_transit_mat(pat_traj_list, from_pat, to_pat):\n",
    "    transit_mat_temp = np.zeros((len(uniq_toks), len(uniq_toks)),int)\n",
    "    for pat in tqdm(pat_traj_list[from_pat:to_pat]):\n",
    "        vis_traj = pat.split(\".\")\n",
    "        for vis_ind1 in range(len(vis_traj)-1):\n",
    "            for code1 in vis_traj[vis_ind1].split(\" \"):\n",
    "                for vis_ind2 in range(vis_ind1+1, len(vis_traj)):\n",
    "                    for code2 in vis_traj[vis_ind2].split(\" \"):\n",
    "                        transit_mat_temp[uniq_toks.index(code1), uniq_toks.index(code2)]+=1\n",
    "    return transit_mat_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2883d3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███                                       | 52/700 [00:58<28:59,  2.68s/it]"
     ]
    }
   ],
   "source": [
    "#use multiprocessing fro calculating the CCS \n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "\n",
    "num_pats = len(vis_seq_df_str)\n",
    "num_cpus =18 \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    pool = multiprocessing.Pool(num_cpus)\n",
    "    start_time = time.perf_counter()\n",
    "    processes = [pool.apply_async(fill_transit_mat, args=(vis_seq_df_str, x, x+700, )) for x in range(0,num_pats,700)]\n",
    "    result = [p.get() for p in processes]\n",
    "    finish_time = time.perf_counter()\n",
    "    print(f\"Program finished in {finish_time-start_time} seconds\")\n",
    "    #print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fa3b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "transit_mat = np.zeros((len(uniq_toks), len(uniq_toks)),int)\n",
    "for res in result:\n",
    "    transit_mat +=res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d8d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save transmat_mat on disk\n",
    "\n",
    "with open('/nfs/home/aliami/causalg2.npy', 'wb') as f:\n",
    "    np.save(f, transit_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0df9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf17c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the transit_mat\n",
    "\n",
    "with open('/nfs/home/aliami/mimic4_dataset/physionet.org/files/mimiciv/2.2/hosp/pre_proc_data/causalg.npy', 'rb') as f:\n",
    "    transit_mat = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1acab69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 4688/4688 [00:14<00:00, 319.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# make the CCS to uni-directional transition matrix by max(0, a(i,j)-a(j,i))\n",
    "transit_mat_one_dir = transit_mat.copy()\n",
    "for i in tqdm(range(transit_mat_one_dir.shape[0])):\n",
    "    for j in range(transit_mat_one_dir.shape[1]):\n",
    "        transit_mat_one_dir[i,j] = max(0, transit_mat_one_dir[i,j]-transit_mat_one_dir[j,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29af3b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smooth the CCS by summing the ccs score for codes with the first 3 digits in common \n",
    "dict_inds_sim = {}\n",
    "dig_granular = 3\n",
    "\n",
    "for ind_tok, uni_tok in enumerate(uniq_toks):\n",
    "    granulae_tok = uni_tok[:dig_granular+2]\n",
    "    if granulae_tok in dict_inds_sim.keys():\n",
    "        dict_inds_sim[granulae_tok].append(ind_tok)\n",
    "    else:\n",
    "        dict_inds_sim[granulae_tok] = [ind_tok]\n",
    "    \n",
    "#add rows and columnith similar first digits\n",
    "transit_mat_light = transit_mat_one_dir.copy()\n",
    "#first sum up similar rows\n",
    "for key, val in dict_inds_sim.items():\n",
    "    temp_row = np.zeros((1,len(uniq_toks)), dtype=int)\n",
    "    for row in val:\n",
    "        temp_row += transit_mat_one_dir[row,:]\n",
    "    for row in val:\n",
    "        transit_mat_light[row, :] = temp_row.copy()\n",
    "        \n",
    "        \n",
    "#then sum up similar columns\n",
    "for key, val in dict_inds_sim.items():\n",
    "    temp_col = np.zeros((1,transit_mat_light.shape[0]), dtype=int)\n",
    "    for col in val:\n",
    "        temp_col += transit_mat_one_dir[:,col]\n",
    "    for col in val:\n",
    "        transit_mat_light[:,col] = temp_col.copy()\n",
    "    \n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90a94453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "953 77\n"
     ]
    }
   ],
   "source": [
    "#Scaling by deviding by ratio between medication and diagnoses\n",
    "\n",
    "count_diag = 0\n",
    "count_med = 0\n",
    "for tok,val in dict_inds_sim.items():\n",
    "    if \"d_\" in tok:\n",
    "        count_diag+=1\n",
    "    else:\n",
    "        count_med+=1\n",
    "print(count_diag, count_med) \n",
    "\n",
    "transit_mat_light = transit_mat_light.astype('float64')\n",
    "\n",
    "scale_factor = .5*count_diag/count_med\n",
    "for ind,tok in enumerate(uniq_toks):\n",
    "    if \"d_\" in tok:\n",
    "        transit_mat_light[ind,:] *= scale_factor\n",
    "\n",
    "for ind,tok in enumerate(uniq_toks):\n",
    "    if \"d_\" in tok:\n",
    "        transit_mat_light[:,ind] *= scale_factor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5fcc04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5f98de9",
   "metadata": {},
   "source": [
    "Prepare data for the following task by assigning to \"main_task\" arguments:\n",
    "\n",
    "    #Mask Language Model: \"MLM\"; \n",
    "    #Conditional Code Swapping  (CCS): \"gcausal_swap_token\";\n",
    "    #Conditional Visit Swapping (CVS):\"gcaus_swap_visit\";\n",
    "    #Random Code Swap (RCS):\"random_swap_token\";\n",
    "    #Random Visit Swap (RVS): \"random_swap_visit\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afc4166",
   "metadata": {},
   "source": [
    "# Build TOO-Bert dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8454d891",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BertDatasetBuilder(Dataset):\n",
    "    CLS = '[CLS]'\n",
    "    PAD = '[PAD]'\n",
    "    SEP = '[sep]'\n",
    "    MASK = '[MASK]'\n",
    "    UNK = '[UNK]'\n",
    "    END = '[end]'\n",
    "\n",
    "    MASK_PERCENTAGE = 0.15\n",
    "\n",
    "    MASKED_INDICES_COLUMN = 'masked_indices'\n",
    "    TARGET_COLUMN = 'indices'\n",
    "    NSP_TARGET_COLUMN = 'is_next'\n",
    "    TOKEN_MASK_COLUMN = 'token_mask'\n",
    "\n",
    "    OPTIMAL_LENGTH_PERCENTILE = 70\n",
    "\n",
    "    def __init__(self, path, input_data,  ds_from=None, ds_to=None, should_include_text=False,\n",
    "                 vocabulary= None, optimal_token_len_per_row=None, single_modal_mask=False, equal_masking=False,\n",
    "                swap_med_diag=False, auxiliary_task=False, main_task=\"MLM\"):\n",
    "        \n",
    "        #valid options for main_task:         \n",
    "        # mlm: \"MLM\"; \n",
    "        #Conditional Code Swapping  (CCS): \"gcausal_swap_token\";\n",
    "        #Conditional Visit Swapping (CVS):\"gcaus_swap_visit\";\n",
    "        #Random Code Swap (RCS):\"random_swap_token\";\n",
    "        #Random Visit Swap (RVS): \"random_swap_visit\"\n",
    "        \n",
    "        \n",
    "        self.ds: pd.Series = input_data#vis_seq_df_str#pd.read_csv(path)['review']\n",
    "\n",
    "        if ds_from is not None or ds_to is not None:\n",
    "            self.ds = self.ds[ds_from:ds_to]\n",
    "\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.counter = Counter()\n",
    "        self.vocab = vocabulary\n",
    "\n",
    "        self.optimal_sentence_length = None\n",
    "        self.optimal_token_len_per_row = optimal_token_len_per_row\n",
    "        self.should_include_text = should_include_text\n",
    "        self.equal_masking = equal_masking\n",
    "        self.swap_med_diag = swap_med_diag\n",
    "        \n",
    "        self.auxiliary_task = auxiliary_task\n",
    "        self.main_task = main_task\n",
    "        \n",
    "        #setting for only masking one modality\n",
    "        #options: False, only_diags, only_meds\n",
    "        self.single_modal_mask = single_modal_mask\n",
    "        #if self.single_modal_mask:\n",
    "            #self.MASK_PERCENTAGE = self.cal_mask_percentage(self.single_modal_mask)\n",
    "            #print(\"for \"+self.single_modal_mask+\" MASK_PERCENTAGE is\"+str(self.MASK_PERCENTAGE))\n",
    "\n",
    "        if should_include_text:\n",
    "            self.columns = ['masked_sentence', self.MASKED_INDICES_COLUMN, 'sentence', self.TARGET_COLUMN,\n",
    "                            self.TOKEN_MASK_COLUMN,\n",
    "                            self.NSP_TARGET_COLUMN]\n",
    "        else:\n",
    "            self.columns = [self.MASKED_INDICES_COLUMN, self.TARGET_COLUMN, self.TOKEN_MASK_COLUMN,\n",
    "                            self.NSP_TARGET_COLUMN]\n",
    "\n",
    "        if self.main_task==\"MLM\":\n",
    "            self.df = self.prepare_dataset()\n",
    "            \n",
    "        if self.main_task==\"lift_based_swap\":\n",
    "            self.complete_df = pd.read_csv(\"complete_df_asso_rule.csv\", index_col=0)[\"Lift\"]\n",
    "            self.df = self.prepare_dataset_swap()\n",
    "            \n",
    "        if self.main_task==\"gcausal_swap_token\" or self.main_task==\"gcaus_swap_visit\":\n",
    "            self.uniq_toks = [uni_code.lower() for uni_code in uniq_toks]\n",
    "            self.items_as_dict = dict(zip(self.uniq_toks,range(0,len(self.uniq_toks))))\n",
    "            self.row_count = 0\n",
    "            self.df = self.prepare_dataset_swap()\n",
    "        if self.main_task==\"random_swap_visit\" or self.main_task==\"random_swap_token\":\n",
    "            #self.uniq_toks = [uni_code.lower() for uni_code in uniq_toks]\n",
    "            #self.items_as_dict = dict(zip(self.uniq_toks,range(0,len(self.uniq_toks))))\n",
    "            #self.row_count = 0\n",
    "            self.df = self.prepare_dataset_swap()\n",
    "        \n",
    "        elif self.main_task==\"swap\":\n",
    "            self.df = self.prepare_dataset_swap()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def cal_mask_percentage(self, single_modal_mask):\n",
    "        num_diags, num_meds = 0, 0\n",
    "        for pat in self.ds:\n",
    "            num_diags += len(re.findall(\"d_\", pat))\n",
    "            num_meds +=  len(re.findall(\"m_\", pat))\n",
    "        perc_mask_med = (.15*num_meds)/(num_diags+ num_meds)\n",
    "        perc_mask_diag = (.15*num_diags)/(num_diags+ num_meds)\n",
    "        if single_modal_mask==\"only_diags\":\n",
    "            return perc_mask_diag\n",
    "        if single_modal_mask==\"only_meds\":\n",
    "            return perc_mask_med\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "\n",
    "        inp = torch.Tensor(item[self.MASKED_INDICES_COLUMN]).long()\n",
    "        token_mask = torch.Tensor(item[self.TOKEN_MASK_COLUMN]).bool()\n",
    "\n",
    "        mask_target = torch.Tensor(item[self.TARGET_COLUMN]).long()\n",
    "        mask_target = mask_target.masked_fill_(token_mask, 0)\n",
    "\n",
    "        attention_mask = (inp == self.vocab[self.PAD]).unsqueeze(0)\n",
    "\n",
    "        if item[self.NSP_TARGET_COLUMN] == 0:\n",
    "            t = [1, 0]\n",
    "        else:\n",
    "            t = [0, 1]\n",
    "\n",
    "        nsp_target = torch.Tensor(t)\n",
    "\n",
    "        return (\n",
    "            inp.to(device),\n",
    "            attention_mask.to(device),\n",
    "            token_mask.to(device),\n",
    "            mask_target.to(device),\n",
    "            nsp_target.to(device)\n",
    "        )\n",
    "    \n",
    "    def prepare_dataset_swap(self) -> pd.DataFrame:\n",
    "        sentences = []\n",
    "        nsp = []\n",
    "        sentence_lens = []\n",
    "        num_codes_per_pat = []\n",
    "\n",
    "        # Split dataset on sentences\n",
    "        for review in self.ds:\n",
    "            num_codes_per_pat.append(len(review.split(\" \")))\n",
    "            review_sentences = review.split('.')\n",
    "            sentences += review_sentences\n",
    "            self._update_length(review_sentences, sentence_lens)\n",
    "        if self.optimal_token_len_per_row == None:\n",
    "            self.optimal_token_len_per_row = int(np.percentile(num_codes_per_pat, \n",
    "                                                               self.OPTIMAL_LENGTH_PERCENTILE))\n",
    "            print(\"optimal_token_len_per_row = \", self.optimal_token_len_per_row)\n",
    "        self.optimal_sentence_length = self._find_optimal_sentence_length(sentence_lens)\n",
    "\n",
    "        print(\"Create vocabulary\")\n",
    "        #ALI: I changed sentences to vis_seq_df_str to build a dictionary with all possible words\n",
    "        for sentence in tqdm(vis_seq_df_str):\n",
    "            s = self.tokenizer(sentence)\n",
    "            self.counter.update(s)\n",
    "\n",
    "        self._fill_vocab()\n",
    "        \n",
    "        \n",
    "        #for having fix size rows based on number of tokens\n",
    "        print(\"Preprocessing dataset\")\n",
    "        for pat_hist in tqdm(self.ds):\n",
    "            pat_hist_toks = pat_hist.replace('.', self.SEP).split(\" \")\n",
    "            #pat_hist_toks += [self.END]\n",
    "            history_len = len(pat_hist_toks)\n",
    "            window_jump = 1\n",
    "            for i in range(0, history_len, window_jump):\n",
    "                \n",
    "                segm = self.tokenizer(' '.join(pat_hist_toks[i :i+self.optimal_token_len_per_row]))\n",
    "                if segm[0]==self.SEP: continue #should not start with sep\n",
    "                #if segm[-1] != self.SEP: segm.append(self.SEP) #add sep at the end of artifacial trajectories\n",
    "                nsp.append(self._create_item(segm, 1, should_masked=False))\n",
    "                \n",
    "                if self.main_task==\"swap\":#self.auxiliary_task == \"swap\":\n",
    "                    #only for trajectories with more than 1 visits and more than 3 uniq tokens (sep, unk, x)\n",
    "                    if segm.count(self.SEP) >1 and len(np.unique(segm))>3:\n",
    "                        #segm_corrupted = self._add_swapping_noise(segm)\n",
    "                        segm_corrupted = self._add_swapping_all_codes_noise(segm)\n",
    "                        nsp.append(self._create_item(segm_corrupted, 0, should_masked=False))\n",
    "                if self.main_task==\"lift_based_swap\":\n",
    "                    #read the prepared association rules mining file\n",
    "                    \n",
    "                    #only for trajectories with more than 1 visits and more than 3 uniq tokens (sep, unk, x)\n",
    "                    if segm.count(self.SEP) >1 and len(np.unique(segm))>3:\n",
    "                        segm_corrupted = self._add_lift_based_swap(segm, .25)\n",
    "                        nsp.append(self._create_item(segm_corrupted, 0, should_masked=False))\n",
    "                #gcausal_swap\n",
    "                if self.main_task==\"gcausal_swap\":\n",
    "                    #read the prepared association rules mining file\n",
    "                    \n",
    "                    #only for trajectories with more than 1 visits and more than 3 uniq tokens (sep, unk, x)\n",
    "                    if segm.count(self.SEP) >1 and len(np.unique(segm))>3:\n",
    "                        segm_corrupted = self._add_gcausal_based_swap(segm, .5)\n",
    "                        nsp.append(self._create_item(segm_corrupted, 0, should_masked=False))\n",
    "                        \n",
    "                if self.main_task==\"gcaus_swap_visit\":\n",
    "                    #only for trajectories with more than 1 visits and more than 3 uniq tokens (sep, unk, x)\n",
    "                    if segm.count(self.SEP) >2 and len(np.unique(segm))>3:\n",
    "                        segm_corrupted = self._add_gcausal_swap_visit(segm, .5)\n",
    "                        nsp.append(self._create_item(segm_corrupted, 0, should_masked=False))\n",
    "                if self.main_task==\"gcausal_swap_token\":\n",
    "                    #read the prepared association rules mining file\n",
    "                    #only for trajectories with more than 1 visits and more than 3 uniq tokens (sep, unk, x)\n",
    "                    if segm.count(self.SEP) >1 and len(np.unique(segm))>3:\n",
    "                        segm_corrupted = self._add_gcausal_based_swap_token(segm, .45)\n",
    "                        nsp.append(self._create_item(segm_corrupted, 0, should_masked=False))\n",
    "                        \n",
    "                if self.main_task==\"random_swap_token\":\n",
    "                    #read the prepared association rules mining file\n",
    "                    \n",
    "                    #only for trajectories with more than 1 visits and more than 3 uniq tokens (sep, unk, x)\n",
    "                    if segm.count(self.SEP) >1 and len(np.unique(segm))>3:\n",
    "                        segm_corrupted = self._add_random_swap_token(segm, .45)\n",
    "                        nsp.append(self._create_item(segm_corrupted, 0, should_masked=False))\n",
    "                        \n",
    "                if self.main_task==\"random_swap_visit\":\n",
    "                    #only for trajectories with more than 1 visits and more than 3 uniq tokens (sep, unk, x)\n",
    "                    if segm.count(self.SEP) >2 and len(np.unique(segm))>3:\n",
    "                        segm_corrupted = self._add_random_swap_visit(segm, 1)\n",
    "                        nsp.append(self._create_item(segm_corrupted, 0, should_masked=False))\n",
    "                        \n",
    "                    \n",
    "                           \n",
    "                \n",
    "                if i+self.optimal_token_len_per_row >= history_len:\n",
    "                    break\n",
    "\n",
    "        df = pd.DataFrame(nsp, columns=self.columns)\n",
    "        return df\n",
    "\n",
    "    def prepare_dataset(self) -> pd.DataFrame:\n",
    "        sentences = []\n",
    "        nsp = []\n",
    "        sentence_lens = []\n",
    "        num_codes_per_pat = []\n",
    "\n",
    "        # Split dataset on sentences\n",
    "        for review in self.ds:\n",
    "            num_codes_per_pat.append(len(review.split(\" \")))\n",
    "            review_sentences = review.split('.')\n",
    "            sentences += review_sentences\n",
    "            self._update_length(review_sentences, sentence_lens)\n",
    "        if self.optimal_token_len_per_row == None:\n",
    "            self.optimal_token_len_per_row = int(np.percentile(num_codes_per_pat, \n",
    "                                                               self.OPTIMAL_LENGTH_PERCENTILE))\n",
    "            print(\"optimal_token_len_per_row = \", self.optimal_token_len_per_row)\n",
    "        self.optimal_sentence_length = self._find_optimal_sentence_length(sentence_lens)\n",
    "\n",
    "        print(\"Create vocabulary\")\n",
    "        #ALI: I changed sentences to vis_seq_df_str to build a dictionary with all possible words\n",
    "        for sentence in tqdm(vis_seq_df_str):\n",
    "            s = self.tokenizer(sentence)\n",
    "            self.counter.update(s)\n",
    "\n",
    "        self._fill_vocab()\n",
    "        \n",
    "        \n",
    "        #for having fix size rows based on number of tokens\n",
    "        print(\"Preprocessing dataset\")\n",
    "        for pat_hist in tqdm(self.ds):\n",
    "            pat_hist_toks = pat_hist.replace('.', self.SEP).split(\" \")\n",
    "            #pat_hist_toks += [self.END]\n",
    "            history_len = len(pat_hist_toks)\n",
    "            window_jump = 1\n",
    "            for i in range(0, history_len, window_jump):\n",
    "                \n",
    "                segm = self.tokenizer(' '.join(pat_hist_toks[i :i+self.optimal_token_len_per_row]))\n",
    "                if segm[0]==self.SEP: continue #should not start with sep\n",
    "                #if segm[-1] != self.SEP: segm.append(self.SEP) #add sep at the end of artifacial trajectories\n",
    "                    \n",
    "                nsp.append(self._create_item(segm, 1, should_masked=True))\n",
    "                \n",
    "                if self.auxiliary_task == \"swap\":\n",
    "                    #only for trajectories with more than 1 visits\n",
    "                    if segm.count(self.SEP) >1:\n",
    "                        segm_corrupted = self._add_swapping_noise(segm)\n",
    "                        nsp.append(self._create_item(segm_corrupted, 0, should_masked=False))\n",
    "                \n",
    "                if i+self.optimal_token_len_per_row >= history_len:\n",
    "                    break\n",
    "\n",
    "        df = pd.DataFrame(nsp, columns=self.columns)\n",
    "        return df\n",
    "        \n",
    "        #for using two sentence instead of a fixed number of tokens\n",
    "        print(\"Preprocessing dataset\")\n",
    "        for review in tqdm(self.ds):\n",
    "            review_sentences = review.split('. ')\n",
    "            if len(review_sentences) > 1:\n",
    "                for i in range(len(review_sentences) - 1):\n",
    "                    # True NSP item\n",
    "                    first, second = self.tokenizer(review_sentences[i]), self.tokenizer(review_sentences[i + 1])\n",
    "                    nsp.append(self._create_item(first, second, 1))\n",
    "\n",
    "                    # False NSP item\n",
    "                    first, second = self._select_false_nsp_sentences(sentences)\n",
    "                    first, second = self.tokenizer(first), self.tokenizer(second)\n",
    "                    nsp.append(self._create_item(first, second, 0))\n",
    "        df = pd.DataFrame(nsp, columns=self.columns)\n",
    "        return df\n",
    "\n",
    "    def _update_length(self, sentences: typing.List[str], lengths: typing.List[int]):\n",
    "        for v in sentences:\n",
    "            l = len(v.split())\n",
    "            lengths.append(l)\n",
    "        return lengths\n",
    "\n",
    "    def _find_optimal_sentence_length(self, lengths: typing.List[int]):\n",
    "        arr = np.array(lengths)\n",
    "        return int(np.percentile(arr, self.OPTIMAL_LENGTH_PERCENTILE))\n",
    "\n",
    "    def _fill_vocab(self):\n",
    "        # specials= argument is only in 0.12.0 version\n",
    "        # specials=[self.CLS, self.PAD, self.MASK, self.SEP, self.UNK]\n",
    "        if self.vocab == None:\n",
    "            self.vocab = vocab(self.counter, min_freq=20)\n",
    "\n",
    "            # 0.11.0 uses this approach to insert specials\n",
    "            self.vocab.insert_token(self.CLS, 0)\n",
    "            self.vocab.insert_token(self.PAD, 1)\n",
    "            self.vocab.insert_token(self.MASK, 2)\n",
    "            self.vocab.insert_token(self.SEP, 3)\n",
    "            self.vocab.insert_token(self.UNK, 4)\n",
    "            #self.vocab.insert_token(self.END, 5)\n",
    "            self.vocab.set_default_index(4)\n",
    "\n",
    "    def _create_item(self, first: typing.List[str], target: int = 1, should_masked=True):\n",
    "        # Create masked sentence item\n",
    "        second = []\n",
    "        updated_first, first_mask = self._preprocess_sentence(first.copy(),should_mask=should_masked)\n",
    "        updated_second, second_mask = self._preprocess_sentence(second.copy(),should_mask=should_masked)\n",
    "\n",
    "        nsp_sentence = updated_first + [self.SEP] + updated_second\n",
    "        nsp_indices = self.vocab.lookup_indices(nsp_sentence)\n",
    "        inverse_token_mask = first_mask + [True] + second_mask\n",
    "\n",
    "        # Create sentence item without masking random words\n",
    "        first, _ = self._preprocess_sentence(first.copy(), should_mask=False)\n",
    "        second, _ = self._preprocess_sentence(second.copy(), should_mask=False)\n",
    "        original_nsp_sentence = first + [self.SEP] + second\n",
    "        original_nsp_indices = self.vocab.lookup_indices(original_nsp_sentence)\n",
    "\n",
    "        if self.should_include_text:\n",
    "            return (\n",
    "                nsp_sentence[:self.optimal_token_len_per_row],\n",
    "                nsp_indices[:self.optimal_token_len_per_row],\n",
    "                original_nsp_sentence[:self.optimal_token_len_per_row],\n",
    "                original_nsp_indices[:self.optimal_token_len_per_row],\n",
    "                inverse_token_mask[:self.optimal_token_len_per_row],\n",
    "                target\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                nsp_indices[:self.optimal_token_len_per_row],\n",
    "                original_nsp_indices[:self.optimal_token_len_per_row],\n",
    "                inverse_token_mask[:self.optimal_token_len_per_row],\n",
    "                target\n",
    "            )\n",
    "\n",
    "    def _select_false_nsp_sentences(self, sentences: typing.List[str]):\n",
    "        \"\"\"Select sentences to create false NSP item\n",
    "\n",
    "        Args:\n",
    "            sentences: list of all sentences\n",
    "\n",
    "        Returns:\n",
    "            tuple of two sentences. The second one NOT the next sentence\n",
    "        \"\"\"\n",
    "        sentences_len = len(sentences)\n",
    "        sentence_index = random.randint(0, sentences_len - 1)\n",
    "        next_sentence_index = random.randint(0, sentences_len - 1)\n",
    "\n",
    "        # To be sure that it's not real next sentence\n",
    "        while next_sentence_index == sentence_index + 1:\n",
    "            next_sentence_index = random.randint(0, sentences_len - 1)\n",
    "\n",
    "        return sentences[sentence_index], sentences[next_sentence_index]\n",
    "\n",
    "    def _preprocess_sentence(self, sentence: typing.List[str], should_mask: bool = True):\n",
    "        inverse_token_mask = []#None\n",
    "        if should_mask:\n",
    "            if self.equal_masking:\n",
    "                sentence, inverse_token_mask = self._mask_sentence_equally(sentence)\n",
    "            elif self.swap_med_diag:\n",
    "                sentence, inverse_token_mask = self._mask_swap_diag_med(sentence)\n",
    "            else:\n",
    "                sentence, inverse_token_mask = self._mask_sentence(sentence, self.single_modal_mask)\n",
    "                \n",
    "        #print(len(inverse_token_mask), inverse_token_mask)\n",
    "        sentence, inverse_token_mask = self._pad_sentence([self.CLS] + sentence, [True]+inverse_token_mask)\n",
    "        return sentence, inverse_token_mask\n",
    "    \n",
    "    def _add_random_swap_token(self, sentence: typing.List[str], swap_percent= .05):\n",
    "        \n",
    "        len_s = len(sentence)\n",
    "        #form pairs for codes between visits:\n",
    "        sep_indx = [i for i in range(len(sentence)) if sentence[i]==self.SEP]\n",
    "        sep_indx.extend([-1, len_s+1])\n",
    "\n",
    "        num_samples = max(1, int(len_s*swap_percent))\n",
    "        #print(\"num_samples\", num_samples)\n",
    "\n",
    "        #pair_toks = []\n",
    "        pair_toks_inds = []\n",
    "        pair_toks_prob = []\n",
    "        for tok_ind,code in enumerate(sentence):\n",
    "            if tok_ind in sep_indx: continue\n",
    "            #should_del_l, should_del_h = max([i for i in sep_indx if i < tok_ind]), min([i for i in sep_indx if i > tok_ind])\n",
    "            should_del_h = min([i for i in sep_indx if i > tok_ind])\n",
    "            should_del_l = 0\n",
    "            available_tok_ind = [i for i in range(len_s) if (i<should_del_l) or (i>should_del_h)]\n",
    "            #remove self.sep from available token list\n",
    "            available_tok_ind = [x for x in available_tok_ind if x not in sep_indx]\n",
    "            for avai_tok in available_tok_ind:\n",
    "                #delete equal pairs\n",
    "                if code==sentence[avai_tok]: continue\n",
    "                #pair_toks.append(str(code)+str(sentence[avai_tok]))\n",
    "                pair_toks_inds.append([tok_ind, avai_tok])\n",
    "\n",
    "                #pair_toks_prob.append(transit_mat_one_dir[self.uniq_toks.index(code), self.uniq_toks.index(sentence[avai_tok])])\n",
    "                #pair_toks_prob.append(transit_mat_one_dir[self.items_as_dict[code], self.items_as_dict[sentence[avai_tok]]])\n",
    "                pair_toks_prob.append(1)    \n",
    "                    \n",
    "        #find the probability of each pair\n",
    "        #build_pair_t = time.time()\n",
    "        #print(\"build_pair_t-start_t\", build_pair_t-start_t)\n",
    "        \n",
    "        #start_t = time.time()\n",
    "        \n",
    "        picked = random.choices(population=pair_toks_inds,weights=pair_toks_prob,k=num_samples)\n",
    "        \n",
    "        #samp_t = time.time()\n",
    "        #print(\"samp_t-start_t\", samp_t-start_t)\n",
    "\n",
    "        #swap tokens elments in sampled list\n",
    "        for samp in picked:\n",
    "            sentence[samp[0]], sentence[samp[1]] = sentence[samp[1]], sentence[samp[0]]\n",
    "        \n",
    "        \n",
    "        return sentence\n",
    "    \n",
    "    \n",
    "    def _add_random_swap_visit(self, sentence: typing.List[str], swap_percent= .05):#it was for random swapping between two visits\n",
    "        \n",
    "        len_s = len(sentence)\n",
    "        #form pairs for codes between visits:\n",
    "        sep_indx = [i for i in range(len(sentence)) if sentence[i]==self.SEP]\n",
    "        sep_indx.insert(0, 0)\n",
    "        sep_indx.append(len_s+1)\n",
    "\n",
    "        n_swap = 1#max(1, int(len_s*swap_percent))\n",
    "        \n",
    "        picked_list = []\n",
    "        for i in range(n_swap):\n",
    "            rand_sel = random.sample(sep_indx[1:-1], 2)#[0]\n",
    "            if rand_sel[1] > rand_sel[0]:\n",
    "                picked = [[rand_sel[0], rand_sel[1]]]\n",
    "            else:\n",
    "                picked = [[rand_sel[1], rand_sel[0]]]\n",
    "            \n",
    "            picked_list.append(picked[0])\n",
    "            \n",
    "        sent_str = \" \".join(sentence)+\" \"\n",
    "        sent_str_vis = sent_str.split(self.SEP)\n",
    "        sent_str_vis = [x+self.SEP for x in sent_str_vis]\n",
    "            \n",
    "        temp = \"@w*\"\n",
    "        for swap_num in range(n_swap):\n",
    "\n",
    "            s1 = sep_indx.index(picked_list[swap_num][0])-1\n",
    "            s2 = sep_indx.index(picked_list[swap_num][1])-1\n",
    "            \n",
    "            \n",
    "\n",
    "            sent_str = sent_str.replace(sent_str_vis[s1], temp,1).replace(sent_str_vis[s2], sent_str_vis[s1],1).replace(temp, sent_str_vis[s2],1)\n",
    "            #s_org.replace(s1, temp).replace(s2, s1).replace(temp, s2)\n",
    "            #if there is \" \" at the first token                \n",
    "            if sent_str[0]==\" \":\n",
    "                    sent_str = sent_str[1:]\n",
    "                \n",
    "            \n",
    "        sent_str = sent_str.replace(\"]m\",\"] m\")\n",
    "        sent_str = sent_str.replace(\"]d\",\"] d\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        return sent_str.split(\" \")[:-1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def _add_gcausal_swap_visit(self, sentence: typing.List[str], swap_percent= .05):\n",
    "        \n",
    "        len_s = len(sentence)\n",
    "        #form pairs for codes between visits:\n",
    "        sep_indx = [i for i in range(len(sentence)) if sentence[i]==self.SEP]\n",
    "        sep_indx.insert(0, 0)\n",
    "        sep_indx.append(len_s+1)\n",
    "\n",
    "        num_samples = 2#max(1, int(len_s*swap_percent))\n",
    "        #print(\"num_samples\", num_samples)\n",
    "        rand_sel = random.sample(sep_indx[1:-1], 2)#[0]\n",
    "        if rand_sel[1] > rand_sel[0]:\n",
    "            picked = [[rand_sel[0], rand_sel[1]]]\n",
    "        else:\n",
    "            picked = [[rand_sel[1], rand_sel[0]]]\n",
    "            \n",
    "        #sel_2 = random.sample([i for i in sep_indx if i>sel_1], 1)[0]\n",
    "        #picked = [[sel_1, sel_2]]\n",
    "        #picked = random.choices(population=pair_visit_ind,weights=pair_visit_prob,k=1)\n",
    "        \n",
    "        #print(pair_visit_ind, pair_visit_prob,picked)\n",
    "        #temp_ind = pair_visit_ind.index(picked[0])\n",
    "        #print(temp_ind, pair_visit_prob[temp_ind])\n",
    "        #samp_t = time.time()\n",
    "        #print(\"samp_t-start_t\", samp_t-start_t)\n",
    "        \n",
    "        #swap the whole visits\n",
    "        start_vis_s, end_vis_s = sep_indx[sep_indx.index(picked[0][0])-1] ,picked[0][0]\n",
    "        #base_inds = [code_ind for code_ind in range(start_vis+1, end_vis)]\n",
    "        \n",
    "        start_vis_t, end_vis_t = sep_indx[sep_indx.index(picked[0][1])-1] ,picked[0][1]\n",
    "        #target_inds = [code_ind for code_ind in range(start_vis+1, end_vis)]\n",
    "        #print(picked)\n",
    "        #print(sentence[-1])\n",
    "        #print(sentence)\n",
    "        #print(sentence[start_vis_s],sentence[end_vis_s],sentence[start_vis_t],sentence[end_vis_t])\n",
    "        #return sentence[:start_vis_s]+sentence[start_vis_t:end_vis_t]+sentence[end_vis_s:start_vis_t]+sentence[start_vis_s:end_vis_s]+sentence[end_vis_t:]\n",
    "        return sentence[:start_vis_s]+sentence[start_vis_t:end_vis_t]+sentence[end_vis_s:start_vis_t]+sentence[start_vis_s:end_vis_s]+sentence[end_vis_t:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _add_gcausal_based_swap_token(self, sentence: typing.List[str], swap_percent= .05):\n",
    "        \n",
    "        len_s = len(sentence)\n",
    "        #form pairs for codes between visits:\n",
    "        sep_indx = [i for i in range(len(sentence)) if sentence[i]==self.SEP]\n",
    "        sep_indx.extend([-1, len_s+1])\n",
    "\n",
    "        num_samples = max(1, int(len_s*swap_percent))\n",
    "        #print(\"num_samples\", num_samples)\n",
    "\n",
    "        #pair_toks = []\n",
    "        pair_toks_inds = []\n",
    "        pair_toks_prob = []\n",
    "        for tok_ind,code in enumerate(sentence):\n",
    "            if tok_ind in sep_indx: continue\n",
    "            #should_del_l, should_del_h = max([i for i in sep_indx if i < tok_ind]), min([i for i in sep_indx if i > tok_ind])\n",
    "            should_del_h = min([i for i in sep_indx if i > tok_ind])\n",
    "            should_del_l = 1\n",
    "            available_tok_ind = [i for i in range(len_s) if (i<should_del_l) or (i>should_del_h)]\n",
    "            #remove self.sep from available token list\n",
    "            available_tok_ind = [x for x in available_tok_ind if x not in sep_indx]\n",
    "            for avai_tok in available_tok_ind:\n",
    "                #delete equal pairs\n",
    "                if code==sentence[avai_tok]: continue\n",
    "                #pair_toks.append(str(code)+str(sentence[avai_tok]))\n",
    "                pair_toks_inds.append([tok_ind, avai_tok])\n",
    "\n",
    "                #pair_toks_prob.append(transit_mat_one_dir[self.uniq_toks.index(code), self.uniq_toks.index(sentence[avai_tok])])\n",
    "                pair_toks_prob.append(transit_mat_one_dir[self.items_as_dict[code], self.items_as_dict[sentence[avai_tok]]])\n",
    "\n",
    "        #find the probability of each pair\n",
    "        #build_pair_t = time.time()\n",
    "        #print(\"build_pair_t-start_t\", build_pair_t-start_t)\n",
    "        \n",
    "        #start_t = time.time()\n",
    "        \n",
    "\n",
    "        \n",
    "        #picked = random.choices(population=pair_toks_inds,weights=pair_toks_prob,k=num_samples)\n",
    "        #add one to avoid zero chance\n",
    "        pair_toks_prob = np.asarray(pair_toks_prob).astype('float64')+1\n",
    "        pair_toks_prob = pair_toks_prob / np.sum(pair_toks_prob)\n",
    "        picked = np.random.choice(len(pair_toks_inds), size=num_samples, replace=False, p=pair_toks_prob)\n",
    "        picked = np.array(pair_toks_inds)[picked]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #samp_t = time.time()\n",
    "        #print(\"samp_t-start_t\", samp_t-start_t)\n",
    "        #print(picked)\n",
    "\n",
    "        #swap tokens elments in sampled list\n",
    "        for samp in picked:\n",
    "            sentence[samp[0]], sentence[samp[1]] = sentence[samp[1]], sentence[samp[0]]\n",
    "        \n",
    "        \n",
    "        return sentence\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def _add_gcausal_swap_visit2(self, sentence: typing.List[str], swap_percent= .05):\n",
    "        \n",
    "        len_s = len(sentence)\n",
    "        #form pairs for codes between visits:\n",
    "        sep_indx = [i for i in range(len(sentence)) if sentence[i]==self.SEP]\n",
    "        sep_indx.insert(0, 0)\n",
    "        sep_indx.append(len_s+1)\n",
    "\n",
    "        num_samples = max(1, int(len_s*swap_percent))\n",
    "        #print(\"num_samples\", num_samples)\n",
    "\n",
    "        #pair_toks = []\n",
    "        pair_toks_inds = []\n",
    "        pair_toks_prob = []\n",
    "        pair_visit_ind = []#[[sep_indx[0], sep_indx[1]]]\n",
    "        pair_visit_prob = []\n",
    "        between_vis_sum_prob = 0\n",
    "        for tok_ind,code in enumerate(sentence):\n",
    "            if tok_ind in sep_indx: continue\n",
    "            #should_del_l, should_del_h = max([i for i in sep_indx if i < tok_ind]), min([i for i in sep_indx if i > tok_ind])\n",
    "            should_del_h = min([i for i in sep_indx if i > tok_ind])\n",
    "            should_del_l = 0\n",
    "            available_tok_ind = [i for i in range(len_s) if (i<should_del_l) or (i>should_del_h)]\n",
    "            #remove self.sep from available token list\n",
    "            #available_tok_ind = [x for x in available_tok_ind if x not in sep_indx]\n",
    "            for avai_tok in available_tok_ind:\n",
    "                if avai_tok in sep_indx:\n",
    "                    if [should_del_h, avai_tok] not in pair_visit_ind:\n",
    "                        pair_visit_ind.append([should_del_h, avai_tok])\n",
    "                        pair_visit_prob.append(between_vis_sum_prob)\n",
    "                        between_vis_sum_prob = 0\n",
    "                    continue\n",
    "\n",
    "                #delete equal pairs\n",
    "                if code==sentence[avai_tok]: continue\n",
    "                pair_toks_inds.append([tok_ind, avai_tok])\n",
    "\n",
    "                #pair_toks_prob.append(transit_mat_light[self.items_as_dict[code], self.items_as_dict[sentence[avai_tok]]])\n",
    "                between_vis_sum_prob += transit_mat_light[self.items_as_dict[code], self.items_as_dict[sentence[avai_tok]]]\n",
    "\n",
    "        #find the probability of each pair\n",
    "        #build_pair_t = time.time()\n",
    "        #print(\"build_pair_t-start_t\", build_pair_t-start_t)\n",
    "        \n",
    "        #start_t = time.time()\n",
    "        \n",
    "        #picked = random.choices(population=pair_toks_inds,weights=pair_toks_prob,k=num_samples)\n",
    "        picked = random.choices(population=pair_visit_ind,weights=pair_visit_prob,k=1)\n",
    "        \n",
    "        #print(pair_visit_ind, pair_visit_prob,picked)\n",
    "        #temp_ind = pair_visit_ind.index(picked[0])\n",
    "        #print(temp_ind, pair_visit_prob[temp_ind])\n",
    "        #samp_t = time.time()\n",
    "        #print(\"samp_t-start_t\", samp_t-start_t)\n",
    "        \n",
    "        #swap the whole visits\n",
    "        start_vis_s, end_vis_s = sep_indx[sep_indx.index(picked[0][0])-1],picked[0][0]\n",
    "        #base_inds = [code_ind for code_ind in range(start_vis+1, end_vis)]\n",
    "        \n",
    "        start_vis_t, end_vis_t = sep_indx[sep_indx.index(picked[0][1])-1],picked[0][1]\n",
    "        #target_inds = [code_ind for code_ind in range(start_vis+1, end_vis)]\n",
    "        #print(picked)\n",
    "        #print(sentence)\n",
    "        #print(sentence[start_vis_s],sentence[end_vis_s],sentence[start_vis_t],sentence[end_vis_t])\n",
    "        return sentence[:start_vis_s]+sentence[start_vis_t:end_vis_t]+sentence[end_vis_s:start_vis_t]+sentence[start_vis_s:end_vis_s]+sentence[end_vis_t:]\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    def _add_gcausal_based_swap(self, sentence: typing.List[str], swap_percent= .05):\n",
    "        \n",
    "        len_s = len(sentence)\n",
    "        #form pairs for codes between visits:\n",
    "        sep_indx = [i for i in range(len(sentence)) if sentence[i]==self.SEP]\n",
    "        sep_indx.extend([-1, len_s+1])\n",
    "\n",
    "        num_samples = max(1, int(len_s*swap_percent))\n",
    "        #print(\"num_samples\", num_samples)\n",
    "\n",
    "        #pair_toks = []\n",
    "        pair_toks_inds = []\n",
    "        pair_toks_prob = []\n",
    "        for tok_ind,code in enumerate(sentence):\n",
    "            if tok_ind in sep_indx: continue\n",
    "            #should_del_l, should_del_h = max([i for i in sep_indx if i < tok_ind]), min([i for i in sep_indx if i > tok_ind])\n",
    "            should_del_h = min([i for i in sep_indx if i > tok_ind])\n",
    "            should_del_l = 0\n",
    "            available_tok_ind = [i for i in range(len_s) if (i<should_del_l) or (i>should_del_h)]\n",
    "            #remove self.sep from available token list\n",
    "            available_tok_ind = [x for x in available_tok_ind if x not in sep_indx]\n",
    "            for avai_tok in available_tok_ind:\n",
    "                #delete equal pairs\n",
    "                if code==sentence[avai_tok]: continue\n",
    "                #pair_toks.append(str(code)+str(sentence[avai_tok]))\n",
    "                pair_toks_inds.append([tok_ind, avai_tok])\n",
    "\n",
    "                #pair_toks_prob.append(transit_mat_one_dir[self.uniq_toks.index(code), self.uniq_toks.index(sentence[avai_tok])])\n",
    "                pair_toks_prob.append(transit_mat_one_dir[self.items_as_dict[code], self.items_as_dict[sentence[avai_tok]]])\n",
    "\n",
    "        #find the probability of each pair\n",
    "        #build_pair_t = time.time()\n",
    "        #print(\"build_pair_t-start_t\", build_pair_t-start_t)\n",
    "        \n",
    "        #start_t = time.time()\n",
    "        \n",
    "        picked = random.choices(population=pair_toks_inds,weights=pair_toks_prob,k=num_samples)\n",
    "        \n",
    "        #samp_t = time.time()\n",
    "        #print(\"samp_t-start_t\", samp_t-start_t)\n",
    "\n",
    "        #swap tokens elments in sampled list\n",
    "        for samp in picked:\n",
    "            sentence[samp[0]], sentence[samp[1]] = sentence[samp[1]], sentence[samp[0]]\n",
    "        \n",
    "        \n",
    "        return sentence\n",
    "    \n",
    "    \n",
    "    def _add_lift_based_swap(self, sentence: typing.List[str], swap_percent= .05):\n",
    "\n",
    "        len_s = len(sentence)\n",
    "        #form pairs for codes between visits:\n",
    "        sep_indx = [i for i in range(len(sentence)) if sentence[i]==self.SEP]\n",
    "        sep_indx.extend([-1, len_s+1])\n",
    "\n",
    "        num_samples = max(1, int(len_s*swap_percent))\n",
    "        #print(\"num_samples\", num_samples)\n",
    "\n",
    "        pair_toks = []\n",
    "        pair_toks_inds = []\n",
    "        for tok_ind,code in enumerate(sentence):\n",
    "            if tok_ind in sep_indx: continue\n",
    "            should_del_l, should_del_h = max([i for i in sep_indx if i < tok_ind]), min([i for i in sep_indx if i > tok_ind])\n",
    "            available_tok_ind = [i for i in range(len_s) if (i<should_del_l) or (i>should_del_h)]\n",
    "            #remove self.sep from available token list\n",
    "            available_tok_ind = [x for x in available_tok_ind if x not in sep_indx]\n",
    "            for avai_tok in available_tok_ind:\n",
    "                #delete equal pairs\n",
    "                if code==sentence[avai_tok]: continue\n",
    "                pair_toks.append(str(code)+str(sentence[avai_tok]))\n",
    "                pair_toks_inds.append([tok_ind, avai_tok])\n",
    "\n",
    "        #find the probability of each pair\n",
    "        \n",
    "        probs_list =  self.complete_df.loc[pair_toks].reset_index()[\"Lift\"]\n",
    "        \n",
    "\n",
    "        picked = random.choices(population=pair_toks_inds,weights=probs_list,k=num_samples)\n",
    "\n",
    "        #swap tokens elments in sampled list\n",
    "        for samp in picked:\n",
    "            sentence[samp[0]], sentence[samp[1]] = sentence[samp[1]], sentence[samp[0]]\n",
    "        \n",
    "        \n",
    "        return sentence\n",
    "\n",
    "    def _add_swapping_all_codes_noise(self, sentence: typing.List[str]):\n",
    "        #instead of swapping only one code, we swapp all the mdeications with all diagnoses on the other visits and so on\n",
    "        \n",
    "        single_source_inp = False\n",
    "        #check if there were tokens only from one modality\n",
    "        if (\"m_\" in \"\".join(sentence)) != (\"d_\" in \"\".join(sentence)):\n",
    "            single_source_inp == True\n",
    "        unusful_vocabs = [self.SEP, self.UNK, self.CLS]\n",
    "        base_ind = 0\n",
    "        #base_token = unusful_vocabs[0]\n",
    "        len_s = len(sentence)\n",
    "        \n",
    "        med_toks_ind = [i for i in range(len(sentence)) if re.match(\"m_\", sentence[i])]\n",
    "        diag_toks_ind = [i for i in range(len(sentence)) if re.match(\"d_\", sentence[i])]\n",
    "        \n",
    "        #reomove all words within the sentence of the base_token to force the swap between sentences \n",
    "        sep_indx = [i for i in range(len(sentence)) if sentence[i]==self.SEP]\n",
    "        #add -1, len(s)+1 to sep_indx for base_ind in first or the last visit\n",
    "        sep_indx.extend([-1, len_s+1])\n",
    "            \n",
    "        \n",
    "        def random_swapping_between_visits(base_ind=False):\n",
    "            #print(\"Random masking\")\n",
    "            if not(base_ind):\n",
    "                while sentence[base_ind] in unusful_vocabs: base_ind=random.sample(range(len_s), k=1)[0];\n",
    "            #print(\"random swap\")\n",
    "            #should_del_l, should_del_h = max([i for i in range(sep_indx) if i >= base_ind]), min([i for i in range(sep_indx) if i >= base_ind])\n",
    "            should_del_l, should_del_h = max([i for i in sep_indx if i < base_ind]), min([i for i in sep_indx if i > base_ind])\n",
    "            available_tok_ind = [i for i in range(len_s) if (i<should_del_l) or (i>should_del_h)]\n",
    "            ##\n",
    "            #for deleting all meds and visits\n",
    "            base_ind_all = [i for i in range(len_s) if (i>should_del_l) and (i<should_del_h)]\n",
    "            #base_ind_all = list(range(should_del_l+1,should_del_h-1))\n",
    "            return swap_ind_selector(base_ind,available_tok_ind,base_ind_all)\n",
    "            ##\n",
    "            swap_ind = base_ind\n",
    "            while sentence[swap_ind] == sentence[base_ind]: swap_ind = random.sample(available_tok_ind, k=1)[0];available_tok_ind.remove(swap_ind)\n",
    "            sentence[base_ind], sentence[swap_ind] = sentence[swap_ind], sentence[base_ind]\n",
    "            return sentence\n",
    "        \n",
    "        def swap_ind_selector(base_ind,available_tokens_list,selected_base_inds):\n",
    "            swap_ind = base_ind\n",
    "            available_tokens_list_copy = available_tokens_list.copy()\n",
    "            while sentence[swap_ind] == sentence[base_ind]:\n",
    "                if len(available_tokens_list)==0:\n",
    "                    return random_swapping_between_visits(base_ind)\n",
    "                swap_ind = random.sample(available_tokens_list, k=1)[0]\n",
    "                available_tokens_list.remove(swap_ind)\n",
    "\n",
    "            \n",
    "            #select all the codes in the selected visits which has the same type with the swap_ind codes\n",
    "            should_del_l, should_del_h = max([i for i in sep_indx if i < swap_ind]), min([i for i in sep_indx if i > swap_ind])\n",
    "            all_swap_tok_inds = [i for i in available_tokens_list_copy if (i>should_del_l) and (i<should_del_h)]\n",
    "\n",
    "            visit_num_base = max([i for i in sep_indx if i < min(selected_base_inds)])\n",
    "            visist_num_swap = max([i for i in sep_indx if i < min(all_swap_tok_inds)])\n",
    "            #remove base_inds from the original sentencse\n",
    "            sentence_base_toks = [sentence[i] for i in selected_base_inds]\n",
    "            sentence_swap_toks = [sentence[i] for i in all_swap_tok_inds]\n",
    "            \n",
    "            #remove sentence_swap_inds from the original sentence\n",
    "            sentence_without_swap_inds = [sentence[i] for i in range(len_s) if sentence[i] not in sentence_swap_toks]\n",
    "            sentence_without_base_inds = [sentence[i] for i in range(len_s) if sentence[i] not in sentence_base_toks]\n",
    "            \n",
    "            if visit_num_base < visist_num_swap:\n",
    "                #swapped_sent = sentence[:visit_num_base]+sentence_swap_toks+sentence_without_base_inds[visit_num_base:visist_num_swap]\n",
    "                middle_toks_without_base = [sentence[i] for i in range(visit_num_base+1,visist_num_swap) if  i not in selected_base_inds]\n",
    "                last_toks_without_swap = [sentence[i] for i in range(visist_num_swap,len_s) if i not in all_swap_tok_inds]\n",
    "                fin_swapped = sentence[:visit_num_base+1]+sentence_swap_toks+middle_toks_without_base+sentence_base_toks+last_toks_without_swap\n",
    "                #print(len(sentence), sentence)\n",
    "                #print(\"selected_base_inds\", selected_base_inds, sentence_base_toks)\n",
    "                #print(\"all_swap_tok_inds\", all_swap_tok_inds, sentence_swap_toks)\n",
    "                #print(\"visit_num_base\", visit_num_base)\n",
    "                #print(\"final_swapped = \", fin_swapped)\n",
    "                return del_duplic_in_visits(fin_swapped)    \n",
    "            else:\n",
    "                middle_toks_without_swap = [sentence[i] for i in range(visist_num_swap+1,visit_num_base) if  i not in all_swap_tok_inds]\n",
    "                last_toks_without_base = [sentence[i] for i in range(visit_num_base,len_s) if i not in selected_base_inds]\n",
    "                fin_swapped = sentence[:visist_num_swap+1]+sentence_base_toks+middle_toks_without_swap+sentence_swap_toks+last_toks_without_base\n",
    "                return del_duplic_in_visits(fin_swapped)\n",
    "            \n",
    "        def del_duplic_in_visits(swapped_sent):\n",
    "\n",
    "            whole_traj = []\n",
    "            visit_codes = []\n",
    "            for i in range(len(swapped_sent)):\n",
    "                visit_codes.append(swapped_sent[i])\n",
    "                if swapped_sent[i]=='[sep]' or i==len(swapped_sent)-1:\n",
    "                    whole_traj.extend(list(OrderedDict.fromkeys(visit_codes)))\n",
    "                    visit_codes.clear()\n",
    "            return whole_traj\n",
    "                    \n",
    "                \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "            #decide 33% swap: med<->diag,or med<->med,or diag<->diag\n",
    "        if not(single_source_inp):\n",
    "                triple_decide = random.random()\n",
    "                #med<->diag\n",
    "                if triple_decide < 0.33:\n",
    "                    #print(\"med<->diag\")\n",
    "                    if len(med_toks_ind)==0:\n",
    "                        return random_swapping_between_visits()\n",
    "                    base_ind=random.sample(med_toks_ind, k=1)[0];\n",
    "                    #should_del_l, should_del_h = max([i for i in range(len_s) if i >= base_ind]), min([i for i in range(len_s) if i >= base_ind])\n",
    "                    should_del_l, should_del_h = max([i for i in sep_indx if i < base_ind]), min([i for i in sep_indx if i > base_ind])\n",
    "                    base_ind_all_meds = [i for i in med_toks_ind if (i>should_del_l) and (i<should_del_h)]\n",
    "                    available_tok_ind = [i for i in range(len_s) if (i<should_del_l) or (i>should_del_h)]\n",
    "                    diag_toks_ind = [i for i in available_tok_ind if re.match(\"d_\", sentence[i])]\n",
    "                    \n",
    "                    if len(diag_toks_ind)==0:\n",
    "                        return random_swapping_between_visits(base_ind)\n",
    "                    else:\n",
    "                        #print(\"med<->diag\")\n",
    "                        return swap_ind_selector(base_ind,diag_toks_ind,base_ind_all_meds)\n",
    "                       \n",
    "                #med<->med\n",
    "                elif triple_decide < 0.66:\n",
    "                    #print(\"med<->med\")\n",
    "                    #base_ind = random.sample(med_toks_ind, k=1)[0]\n",
    "                    if len(med_toks_ind)==0:\n",
    "                        return random_swapping_between_visits()\n",
    "                    base_ind=random.sample(med_toks_ind, k=1)[0];\n",
    "                    #should_del_l, should_del_h = max([i for i in range(len_s) if i >= base_ind]), min([i for i in range(len_s) if i >= base_ind])\n",
    "                    should_del_l, should_del_h = max([i for i in sep_indx if i < base_ind]), min([i for i in sep_indx if i > base_ind])\n",
    "                    base_ind_all_meds = [i for i in med_toks_ind if (i>should_del_l) and (i<should_del_h)]\n",
    "                    available_tok_ind = [i for i in range(len_s) if (i<should_del_l) or (i>should_del_h)]\n",
    "                    med_toks_ind = [i for i in available_tok_ind if re.match(\"m_\", sentence[i])]\n",
    "                    if len(med_toks_ind)==0:\n",
    "                        return random_swapping_between_visits(base_ind)\n",
    "                    else:\n",
    "                        #print(\"med<->med\")\n",
    "                        return swap_ind_selector(base_ind,med_toks_ind,base_ind_all_meds)\n",
    "                        #return swap_ind_selector(base_ind,med_toks_ind)\n",
    "                        \n",
    "                #diag<->diag\n",
    "                else:\n",
    "                    #print(\"diag<->diag\")\n",
    "                    #base_ind = random.sample(diag_toks_ind, k=1)[0]\n",
    "                    if len(diag_toks_ind)==0:\n",
    "                        return random_swapping_between_visits()\n",
    "                    base_ind=random.sample(diag_toks_ind, k=1)[0];\n",
    "                    #should_del_l, should_del_h = max([i for i in range(len_s) if i >= base_ind]), min([i for i in range(len_s) if i >= base_ind])\n",
    "                    should_del_l, should_del_h = max([i for i in sep_indx if i < base_ind]), min([i for i in sep_indx if i > base_ind])\n",
    "                    base_ind_all_diags = [i for i in diag_toks_ind if (i>should_del_l) and (i<should_del_h)]\n",
    "                    available_tok_ind = [i for i in range(len_s) if (i<should_del_l) or (i>should_del_h)]\n",
    "                    diag_toks_ind = [i for i in available_tok_ind if re.match(\"d_\", sentence[i])]\n",
    "                    if len(diag_toks_ind)==0:\n",
    "                        return random_swapping_between_visits(base_ind)\n",
    "                    else:\n",
    "                        #print(\"diag<->diag\")\n",
    "                        return swap_ind_selector(base_ind,diag_toks_ind,base_ind_all_diags)\n",
    "                        #return swap_ind_selector(base_ind,diag_toks_ind)\n",
    "                        #swap_ind = random.sample(diag_toks_ind, k=1)[0]\n",
    "                        #swap_ind = base_ind\n",
    "                        #while sentence[swap_ind] == sentence[base_ind]: swap_ind = random.sample(diag_toks_ind, k=1)[0];diag_toks_ind.remove(swap_ind)\n",
    "                        #sentence[base_ind], sentence[swap_ind] = sentence[swap_ind], sentence[base_ind]\n",
    "        else:\n",
    "            #print(\"single modal\")\n",
    "            random_swapping_between_visits()\n",
    "            \n",
    "            \n",
    "        return sentence\n",
    "    \n",
    "    \n",
    "    def _add_swapping_noise(self, sentence: typing.List[str]):\n",
    "        single_source_inp = False\n",
    "        #check if there were tokens only from one modality\n",
    "        if (\"m_\" in \"\".join(sentence)) != (\"d_\" in \"\".join(sentence)):\n",
    "            single_source_inp == True\n",
    "        unusful_vocabs = [self.SEP, self.UNK, self.CLS]\n",
    "        base_ind = 0\n",
    "        #base_token = unusful_vocabs[0]\n",
    "        len_s = len(sentence)\n",
    "        \n",
    "        med_toks_ind = [i for i in range(len(sentence)) if re.match(\"m_\", sentence[i])]\n",
    "        diag_toks_ind = [i for i in range(len(sentence)) if re.match(\"d_\", sentence[i])]\n",
    "        \n",
    "        #reomove all words within the sentence of the base_token to force the swap between sentences \n",
    "        sep_indx = [i for i in range(len(sentence)) if sentence[i]==self.SEP]\n",
    "        #add -1, len(s)+1 to sep_indx for base_ind in first or the last visit\n",
    "        sep_indx.extend([-1, len_s+1])\n",
    "            \n",
    "        \n",
    "        def random_swapping_between_visits(base_ind=False):\n",
    "            #print(\"Random masking\")\n",
    "            if not(base_ind):\n",
    "                while sentence[base_ind] in unusful_vocabs: base_ind=random.sample(range(len_s), k=1)[0];\n",
    "            #print(\"random swap\")\n",
    "            #should_del_l, should_del_h = max([i for i in range(sep_indx) if i >= base_ind]), min([i for i in range(sep_indx) if i >= base_ind])\n",
    "            should_del_l, should_del_h = max([i for i in sep_indx if i < base_ind]), min([i for i in sep_indx if i > base_ind])\n",
    "            available_tok_ind = [i for i in range(len_s) if (i<should_del_l) or (i>should_del_h)]\n",
    "            #swap_ind = random.sample(available_tok_ind, k=1)[0]\n",
    "            swap_ind = base_ind\n",
    "            while sentence[swap_ind] == sentence[base_ind]: swap_ind = random.sample(available_tok_ind, k=1)[0];available_tok_ind.remove(swap_ind)\n",
    "            sentence[base_ind], sentence[swap_ind] = sentence[swap_ind], sentence[base_ind]\n",
    "            return sentence\n",
    "        \n",
    "        def swap_ind_selector(base_ind,available_tokens_list):\n",
    "            swap_ind = base_ind\n",
    "            while sentence[swap_ind] == sentence[base_ind]:\n",
    "                if len(available_tokens_list)==0:\n",
    "                    return random_swapping_between_visits(base_ind)\n",
    "                swap_ind = random.sample(available_tokens_list, k=1)[0]\n",
    "                available_tokens_list.remove(swap_ind)\n",
    "                \n",
    "            sentence[base_ind], sentence[swap_ind] = sentence[swap_ind], sentence[base_ind]\n",
    "            return sentence\n",
    "        \n",
    "        \n",
    "            #decide 33% swap: med<->diag,or med<->med,or diag<->diag\n",
    "        if not(single_source_inp):\n",
    "                triple_decide = random.random()\n",
    "                #med<->diag\n",
    "                if triple_decide < 0.33:\n",
    "                    #print(\"med<->diag\")\n",
    "                    #base_ind = random.sample(med_toks_ind, k=1)[0]\n",
    "                    if len(med_toks_ind)==0:\n",
    "                        return random_swapping_between_visits()\n",
    "                    base_ind=random.sample(med_toks_ind, k=1)[0];\n",
    "                    #should_del_l, should_del_h = max([i for i in range(len_s) if i >= base_ind]), min([i for i in range(len_s) if i >= base_ind])\n",
    "                    should_del_l, should_del_h = max([i for i in sep_indx if i < base_ind]), min([i for i in sep_indx if i > base_ind])\n",
    "                    available_tok_ind = [i for i in range(len_s) if (i<should_del_l) or (i>should_del_h)]\n",
    "                    diag_toks_ind = [i for i in available_tok_ind if re.match(\"d_\", sentence[i])]\n",
    "                    if len(diag_toks_ind)==0:\n",
    "                        return random_swapping_between_visits(base_ind)\n",
    "                    else:\n",
    "                        #print(\"med<->diag\")\n",
    "                        return swap_ind_selector(base_ind,diag_toks_ind)\n",
    "                        #swap_ind = random.sample(diag_toks_ind, k=1)[0]\n",
    "                        #swap_ind = base_ind\n",
    "                        #while sentence[swap_ind] == sentence[base_ind]: swap_ind = random.sample(diag_toks_ind, k=1)[0];diag_toks_ind.remove(swap_ind)\n",
    "                        #sentence[base_ind], sentence[swap_ind] = sentence[swap_ind], sentence[base_ind]\n",
    "                #med<->med\n",
    "                elif triple_decide < 0.66:\n",
    "                    #print(\"med<->med\")\n",
    "                    #base_ind = random.sample(med_toks_ind, k=1)[0]\n",
    "                    if len(med_toks_ind)==0:\n",
    "                        return random_swapping_between_visits()\n",
    "                    base_ind=random.sample(med_toks_ind, k=1)[0];\n",
    "                    #should_del_l, should_del_h = max([i for i in range(len_s) if i >= base_ind]), min([i for i in range(len_s) if i >= base_ind])\n",
    "                    should_del_l, should_del_h = max([i for i in sep_indx if i < base_ind]), min([i for i in sep_indx if i > base_ind])\n",
    "                    available_tok_ind = [i for i in range(len_s) if (i<should_del_l) or (i>should_del_h)]\n",
    "                    med_toks_ind = [i for i in available_tok_ind if re.match(\"m_\", sentence[i])]\n",
    "                    if len(med_toks_ind)==0:\n",
    "                        return random_swapping_between_visits(base_ind)\n",
    "                    else:\n",
    "                        #print(\"med<->med\")\n",
    "                        return swap_ind_selector(base_ind,med_toks_ind)\n",
    "                        #swap_ind = random.sample(med_toks_ind, k=1)[0]\n",
    "                        #swap_ind = base_ind\n",
    "                        #while sentence[swap_ind] == sentence[base_ind]: swap_ind = random.sample(med_toks_ind, k=1)[0];med_toks_ind.remove(swap_ind)\n",
    "                        #sentence[base_ind], sentence[swap_ind] = sentence[swap_ind], sentence[base_ind]\n",
    "                #diag<->diag\n",
    "                else:\n",
    "                    #print(\"diag<->diag\")\n",
    "                    #base_ind = random.sample(diag_toks_ind, k=1)[0]\n",
    "                    if len(diag_toks_ind)==0:\n",
    "                        return random_swapping_between_visits()\n",
    "                    base_ind=random.sample(diag_toks_ind, k=1)[0];\n",
    "                    #should_del_l, should_del_h = max([i for i in range(len_s) if i >= base_ind]), min([i for i in range(len_s) if i >= base_ind])\n",
    "                    should_del_l, should_del_h = max([i for i in sep_indx if i < base_ind]), min([i for i in sep_indx if i > base_ind])\n",
    "                    available_tok_ind = [i for i in range(len_s) if (i<should_del_l) or (i>should_del_h)]\n",
    "                    diag_toks_ind = [i for i in available_tok_ind if re.match(\"d_\", sentence[i])]\n",
    "                    if len(diag_toks_ind)==0:\n",
    "                        return random_swapping_between_visits(base_ind)\n",
    "                    else:\n",
    "                        #print(\"diag<->diag\")\n",
    "                        return swap_ind_selector(base_ind,diag_toks_ind)\n",
    "                        #swap_ind = random.sample(diag_toks_ind, k=1)[0]\n",
    "                        #swap_ind = base_ind\n",
    "                        #while sentence[swap_ind] == sentence[base_ind]: swap_ind = random.sample(diag_toks_ind, k=1)[0];diag_toks_ind.remove(swap_ind)\n",
    "                        #sentence[base_ind], sentence[swap_ind] = sentence[swap_ind], sentence[base_ind]\n",
    "        else:\n",
    "            #print(\"single modal\")\n",
    "            random_swapping_between_visits()\n",
    "            \n",
    "            \n",
    "        return sentence\n",
    "            \n",
    "        \n",
    "                    \n",
    "                    \n",
    "        \n",
    "        while base_token in unusful_vocabs: base_ind=random.randint(0, len_s - 1); base_token = sentence[base_ind]\n",
    "            \n",
    "        #reomove all words within the sentence of the base_token to force the swap between sentences \n",
    "        sep_indx = [i for i in range(len(sentence)) if sentence[i]==self.SEP]\n",
    "        should_del_l, should_del_h = max([i for i in a if i >= base_ind]), min([i for i in a if i >= base_ind])\n",
    "        available_tok_ind = [i for i in range(len_s) if (i<should_del_l) or (i>should_del_h)]\n",
    "    \n",
    "        med_toks_ind = [i for i in available_tok_ind if re.match(\"m_\", sentence[i])]\n",
    "        diag_toks_ind = [i for i in available_tok_ind if re.match(\"d_\", sentence[i])]\n",
    "        \n",
    "        #if len(med_toks_ind)\n",
    "        \n",
    "        #print(base_ind, base_token, sentence)\n",
    "        #print(zzz)\n",
    "            \n",
    "\n",
    "    def _mask_sentence(self, sentence: typing.List[str], single_modal_mask=False, remain_amount_mask=False):\n",
    "        #print(\"sentence = \", sentence)\n",
    "        \"\"\"Replace MASK_PERCENTAGE (15%) of words with special [MASK] symbol\n",
    "        or with random word from vocabulary\n",
    "\n",
    "        Args:\n",
    "            sentence: sentence to process\n",
    "\n",
    "        Returns:\n",
    "            tuple of processed sentence and inverse token mask\n",
    "        \"\"\"\n",
    "        unusful_vocabs = [self.SEP, self.UNK, self.CLS]\n",
    "        \n",
    "        len_s = len(sentence)\n",
    "        inverse_token_mask = [True for _ in range(max(len_s, self.optimal_token_len_per_row))]\n",
    "\n",
    "        if remain_amount_mask:\n",
    "            mask_amount = remain_amount_mask\n",
    "        else:\n",
    "            mask_amount = round(len_s * self.MASK_PERCENTAGE)\n",
    "        \n",
    "        #print(\"mask_amount = \", mask_amount)\n",
    "        #for _ in range(mask_amount):\n",
    "        masked_count = 0\n",
    "        while masked_count<mask_amount:\n",
    "            i = random.randint(0, len_s - 1)\n",
    "            \n",
    "            #since learning the sep is not useful we dont mask them!\n",
    "            if sentence[i] in unusful_vocabs:\n",
    "                continue\n",
    "            if single_modal_mask == \"only_diags\":\n",
    "                #check if there is any token of the desired modal\n",
    "                if not(\"d_\" in \"\".join(sentence)):\n",
    "                    single_modal_mask = None\n",
    "                #check if the right modal is masked       \n",
    "                if re.match(\"m_\", sentence[i]):\n",
    "                    continue\n",
    "                       \n",
    "            if single_modal_mask == \"only_meds\":\n",
    "                #check if there is any token of the desired modal\n",
    "                if not(\"m_\" in \"\".join(sentence)):\n",
    "                    single_modal_mask = None\n",
    "                #check if the right modal is masked \n",
    "                if re.match(\"d_\", sentence[i]):\n",
    "                    continue\n",
    "                    \n",
    "                    \n",
    "            #decide in 80% of moment with MASk tokens, 10% with a random word and 10% keep it without change\n",
    "            triple_decide = random.random()\n",
    "            if triple_decide < 0.8:\n",
    "                sentence[i] = self.MASK\n",
    "            elif triple_decide < 0.9:\n",
    "                # All is below 5 is special token\n",
    "                # see self._insert_specials method\n",
    "                j = random.randint(5, len(self.vocab) - 1)\n",
    "                sentence[i] = self.vocab.lookup_token(j)\n",
    "            else:\n",
    "                pass\n",
    "            inverse_token_mask[i] = False\n",
    "            masked_count+=1\n",
    "        \n",
    "        #print(\"sentence after masking\", sentence,inverse_token_mask)\n",
    "        return sentence, inverse_token_mask\n",
    "    \n",
    "    def _mask_sentence_equally(self, sentence: typing.List[str], single_modal_mask=\"only_meds\"):\n",
    "        #print(\"sentence = \", sentence)\n",
    "        \n",
    "        \"\"\"\n",
    "        Mask equally between diagnosis and medication\n",
    "        Replace MASK_PERCENTAGE (15%) of words with special [MASK] symbol\n",
    "        or with random word from vocabulary\n",
    "\n",
    "        Args:\n",
    "            sentence: sentence to process\n",
    "\n",
    "        Returns:\n",
    "            tuple of processed sentence and inverse token mask\n",
    "        \"\"\"\n",
    "        \n",
    "        unusful_vocabs = [self.SEP, self.UNK, self.CLS]\n",
    "\n",
    "        \n",
    "        len_s = len(sentence)\n",
    "        inverse_token_mask = [True for _ in range(max(len_s, self.optimal_token_len_per_row))]\n",
    "        change_single_modal_flag = False\n",
    "\n",
    "        mask_amount = round(len_s * self.MASK_PERCENTAGE)\n",
    "        #for _ in range(mask_amount):\n",
    "        masked_count = 0\n",
    "        while masked_count<mask_amount:\n",
    "            #print(masked_count,mask_amount,single_modal_mask)\n",
    "            i = random.randint(0, len_s - 1)\n",
    "            \n",
    "            #check if there were tokens only from one modality\n",
    "            if (\"m_\" in \"\".join(sentence)) != (\"d_\" in \"\".join(sentence)):\n",
    "                single_modal_mask = None\n",
    "\n",
    "            #since learning the sep is not useful we dont mask them!\n",
    "            if sentence[i] in unusful_vocabs:\n",
    "                continue\n",
    "            if single_modal_mask == \"only_diags\":\n",
    "                if re.match(\"m_\", sentence[i]):\n",
    "                    continue\n",
    "            if single_modal_mask == \"only_meds\":\n",
    "                if re.match(\"d_\", sentence[i]):\n",
    "                    continue\n",
    "                    \n",
    "            #print(single_modal_mask, sentence[i])        \n",
    "            #decide in 80% of moment with MASk tokens, 10% with a random word and 10% keep it without change\n",
    "            triple_decide = random.random()\n",
    "            if triple_decide < 0.8:\n",
    "                sentence[i] = self.MASK\n",
    "            elif triple_decide < 0.9:\n",
    "                # All is below 5 is special token\n",
    "                # see self._insert_specials method\n",
    "                j = random.randint(5, len(self.vocab) - 1)\n",
    "                sentence[i] = self.vocab.lookup_token(j)\n",
    "            else:\n",
    "                pass\n",
    "            inverse_token_mask[i] = False\n",
    "            masked_count+=1\n",
    "            #mask equally by repeatedly changing to only diags and only meds\n",
    "            #single_modal_mask = \"only_meds\" if single_modal_mask == \"only_diags\" else \"only_diags\"\n",
    "            if single_modal_mask==\"only_diags\":\n",
    "                single_modal_mask=\"only_meds\"\n",
    "            elif single_modal_mask==\"only_meds\":\n",
    "                single_modal_mask=\"only_diags\"\n",
    "\n",
    "            \n",
    "            \n",
    "        #print(\"sentence after masking\", sentence)\n",
    "        return sentence, inverse_token_mask\n",
    "    \n",
    "    def _mask_swap_diag_med(self, sentence: typing.List[str], single_modal_mask=False):\n",
    "        #print(\"sentence = \", sentence)\n",
    "        \n",
    "        \"\"\"\n",
    "        Mask equally between diagnosis and medication\n",
    "        Replace MASK_PERCENTAGE (15%) of words with special [MASK] symbol\n",
    "        or with random word from vocabulary\n",
    "\n",
    "        Args:\n",
    "            sentence: sentence to process\n",
    "\n",
    "        Returns:\n",
    "            tuple of processed sentence and inverse token mask\n",
    "        \"\"\"\n",
    "        unusful_vocabs = [self.SEP, self.UNK, self.CLS]\n",
    "        \n",
    "        len_s = len(sentence)\n",
    "        inverse_token_mask = [True for _ in range(max(len_s, self.optimal_token_len_per_row))]\n",
    "        change_single_modal_flag = False\n",
    "\n",
    "        med_toks_ind = [i for i in range(len(sentence)) if re.match(\"m_\", sentence[i])]\n",
    "        diag_toks_ind = [i for i in range(len(sentence)) if re.match(\"d_\", sentence[i])]\n",
    "        \n",
    "        \n",
    "        mask_amount = round((len_s * (self.MASK_PERCENTAGE))/2)\n",
    "        masked_count = 0\n",
    "        sentence_check_temp = sentence.copy()\n",
    "        while masked_count<mask_amount:\n",
    "            i = random.randint(0, len_s - 1)\n",
    "            \n",
    "            #check if there were tokens only from one modality, then mask some tokens normally\n",
    "            if (len(diag_toks_ind)==0) or (len(med_toks_ind)==0):\n",
    "                return self._mask_sentence(sentence,\n",
    "                                           single_modal_mask=False,\n",
    "                                           remain_amount_mask=mask_amount-masked_count)\n",
    "            \n",
    "            #since learning the sep is not useful we dont mask them!\n",
    "            if sentence[i] in unusful_vocabs:\n",
    "                if sentence[i]==self.CLS:\n",
    "                    print(sentence[i])\n",
    "                continue\n",
    "                \n",
    "            if re.match(\"m_\", sentence[i]):\n",
    "                rand_diag_cod = random.sample(diag_toks_ind, k=1)[0]\n",
    "                #sampling without replacement\n",
    "                diag_toks_ind.remove(rand_diag_cod)\n",
    "                sentence[i], sentence[rand_diag_cod] = sentence[rand_diag_cod], sentence[i]\n",
    "                inverse_token_mask[rand_diag_cod] = False\n",
    "                \n",
    "            if re.match(\"d_\", sentence[i]):\n",
    "                rand_med_cod = random.sample(med_toks_ind, k=1)[0]\n",
    "                #sampling without replacement\n",
    "                med_toks_ind.remove(rand_med_cod)\n",
    "                sentence[i], sentence[rand_med_cod] = sentence[rand_med_cod], sentence[i]\n",
    "                inverse_token_mask[rand_med_cod] = False\n",
    "                \n",
    "            inverse_token_mask[i] = False\n",
    "            #unchanged_tokens = diag_toks_ind+med_toks_ind\n",
    "            #unchanged_tokens.sort()\n",
    "            #sentence_check_temp = [sentence[i] for i in unchanged_tokens]\n",
    "            masked_count+=1\n",
    "            \n",
    "            \n",
    "        #print(\"sentence after masking\", sentence)\n",
    "        return sentence, inverse_token_mask\n",
    "\n",
    "    def _pad_sentence(self, sentence: typing.List[str], inverse_token_mask: typing.List[bool] = None):\n",
    "        len_s = len(sentence)\n",
    "        #print(\"self.optimal_sentence_length = \", self.optimal_sentence_length)\n",
    "\n",
    "        #if len_s >= self.optimal_sentence_length:\n",
    "        if len_s >= self.optimal_token_len_per_row:\n",
    "            s = sentence[:self.optimal_token_len_per_row]\n",
    "        else:\n",
    "            s = sentence + [self.PAD] * (self.optimal_token_len_per_row - len_s)\n",
    "\n",
    "        # inverse token mask should be padded as well\n",
    "        if inverse_token_mask:\n",
    "            len_m = len(inverse_token_mask)\n",
    "            if len_m >= self.optimal_token_len_per_row:\n",
    "                inverse_token_mask = inverse_token_mask[:self.optimal_token_len_per_row]\n",
    "            else:\n",
    "                inverse_token_mask = inverse_token_mask + [True] * (self.optimal_token_len_per_row - len_m)\n",
    "        #print(\"Sentence after padding = \", s)\n",
    "        return s, inverse_token_mask\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    __file__ = \".\"\n",
    "    BASE_DIR = Path(__file__).resolve().parent.parent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b58a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDatasetAggregate(BertDatasetBuilder):\n",
    "    \n",
    "    def __init__(self, list_bert_ds_bulders):\n",
    "        #super().__init__(path, input_data)\n",
    "        self.list_bert_ds_bulders = list_bert_ds_bulders\n",
    "        self.df = self.merge_bert_dataset_ojs(self.list_bert_ds_bulders)\n",
    "        print(\"the shape of final dataframe is \", self.df.shape)\n",
    "        self.optimal_sentence_length = self.list_bert_ds_bulders[0].optimal_sentence_length\n",
    "        self.vocab = self.list_bert_ds_bulders[0].vocab\n",
    "\n",
    "    def merge_bert_dataset_ojs(self, list_ds_objs):\n",
    "        sample_size = int(list_ds_objs[0].df.shape[0]/len(list_ds_objs))\n",
    "        list_dfs = [obj.df.sample(sample_size) for obj in list_ds_objs]\n",
    "        final_df = pd.concat(list_dfs, axis=0, ignore_index=True)\n",
    "        return final_df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "\n",
    "        inp = torch.Tensor(item[self.MASKED_INDICES_COLUMN]).long()\n",
    "        token_mask = torch.Tensor(item[self.TOKEN_MASK_COLUMN]).bool()\n",
    "\n",
    "        mask_target = torch.Tensor(item[self.TARGET_COLUMN]).long()\n",
    "        mask_target = mask_target.masked_fill_(token_mask, 0)\n",
    "\n",
    "        attention_mask = (inp == self.vocab[self.PAD]).unsqueeze(0)\n",
    "\n",
    "        if item[self.NSP_TARGET_COLUMN] == 0:\n",
    "            t = [1, 0]\n",
    "        else:\n",
    "            t = [0, 1]\n",
    "\n",
    "        nsp_target = torch.Tensor(t)\n",
    "\n",
    "        return (\n",
    "            inp.to(device),\n",
    "            attention_mask.to(device),\n",
    "            token_mask.to(device),\n",
    "            mask_target.to(device),\n",
    "            nsp_target.to(device)\n",
    "        )\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3a502f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8885861d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal_token_len_per_row =  56\n",
      "Create vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 52832/52832 [00:01<00:00, 34820.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 200/200 [00:00<00:00, 636.18it/s]\n"
     ]
    }
   ],
   "source": [
    "#main_task = MLM, CVS:gcaus_swap_visit, CCS: gcausal_swap_token\n",
    "# Random CS (RCS): random_swap_token\n",
    "#Random VS (RVS): \"random_swap_visit\"\n",
    "\n",
    "ds_swap = BertDatasetBuilder(BASE_DIR.joinpath('data/imdb.csv'), input_data=X_train, ds_from=0, ds_to=200,\n",
    "                        should_include_text=True, main_task=\"random_swap_visit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c43796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f54ed799",
   "metadata": {},
   "source": [
    "# Transformer NN architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d237e77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class JointEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, size, ds_vocab):\n",
    "        super(JointEmbedding, self).__init__()\n",
    "\n",
    "        self.ds_vocab = ds_vocab\n",
    "        self.size = size\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, size)\n",
    "        self.segment_emb = nn.Embedding(vocab_size, size)\n",
    "\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        sentence_size = input_tensor.size(-1)\n",
    "        pos_tensor = self.attention_position(self.size, input_tensor)\n",
    "        visit_pos_tensor = self.visits_position(self.size, input_tensor)\n",
    "        visit_atten_pos_sin_cos = self.visit_attention_position_sin_cos(self.size,visit_pos_tensor)\n",
    "\n",
    "        segment_tensor = torch.zeros_like(input_tensor).to(device)\n",
    "        segment_tensor[:, sentence_size // 2 + 1:] = 1\n",
    "        #deleted by Ali: \"pos_tensor\" inorder to exclude the order of codes in the same visit\n",
    "        output = self.token_emb(input_tensor) +  self.segment_emb(visit_pos_tensor) + visit_atten_pos_sin_cos# + self.segment_emb(segment_tensor) #+ pos_tensor\n",
    "        return self.norm(output)\n",
    "    \n",
    "    def visits_position(self, dim, input_tensor):\n",
    "        batch_size = input_tensor.size(0)\n",
    "        #print(\"size \", input_tensor.shape)\n",
    "        sep_to_id = self.ds_vocab.lookup_indices(['[sep]'])[0]\n",
    "        batch_vis_nums = []\n",
    "        for row in input_tensor:\n",
    "            #print(row , sep_to_id)\n",
    "            vis_count = 0\n",
    "            vis_nums = []\n",
    "            for code in row:\n",
    "                if code == sep_to_id:\n",
    "                    vis_count+=1\n",
    "                vis_nums.append(vis_count)\n",
    "            batch_vis_nums.append(vis_nums)\n",
    "        return(torch.tensor(np.array(batch_vis_nums))).to(device)\n",
    "\n",
    "        \n",
    "    \n",
    "    def visit_attention_position_sin_cos(self, dim, input_tensor):\n",
    "        batch_size = input_tensor.size(0)\n",
    "        sentence_size = input_tensor.size(-1)\n",
    "\n",
    "        #pos = torch.arange(sentence_size, dtype=torch.long).to(device)\n",
    "        \n",
    "        even_i = torch.arange(0, dim, 2).float()\n",
    "        denominator = torch.pow(10000, even_i/dim).to(device)\n",
    "        \n",
    "        position = input_tensor.reshape(sentence_size*batch_size, 1)#torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        PE2 = torch.reshape(PE, (batch_size,sentence_size,dim))\n",
    "        #print(input_tensor.shape, PE.shape, PE2.shape)\n",
    "        return PE2\n",
    "\n",
    "\n",
    "    def attention_position(self, dim, input_tensor):\n",
    "        batch_size = input_tensor.size(0)\n",
    "        sentence_size = input_tensor.size(-1)\n",
    "\n",
    "        pos = torch.arange(sentence_size, dtype=torch.long).to(device)\n",
    "        d = torch.arange(dim, dtype=torch.long).to(device)\n",
    "        d = (2 * d / dim)\n",
    "\n",
    "        pos = pos.unsqueeze(1)\n",
    "        pos = pos / (1e4 ** d)\n",
    "\n",
    "        pos[:, ::2] = torch.sin(pos[:, ::2])\n",
    "        pos[:, 1::2] = torch.cos(pos[:, 1::2])\n",
    "\n",
    "        return pos.expand(batch_size, *pos.size())\n",
    "\n",
    "    def numeric_position(self, dim, input_tensor):\n",
    "        pos_tensor = torch.arange(dim, dtype=torch.long).to(device)\n",
    "        return pos_tensor.expand_as(input_tensor)\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_inp, dim_out):\n",
    "        super(AttentionHead, self).__init__()\n",
    "\n",
    "        self.dim_inp = dim_inp\n",
    "\n",
    "        self.q = nn.Linear(dim_inp, dim_out)\n",
    "        self.k = nn.Linear(dim_inp, dim_out)\n",
    "        self.v = nn.Linear(dim_inp, dim_out)\n",
    "        self.idaten = nn.Identity()\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor = None):\n",
    "        query, key, value = self.q(input_tensor), self.k(input_tensor), self.v(input_tensor)\n",
    "\n",
    "        scale = query.size(1) ** 0.5\n",
    "        scores = torch.bmm(query, key.transpose(1, 2)) / scale\n",
    "        #print(\"scores\", scores.shape)\n",
    "\n",
    "        scores = scores.masked_fill_(attention_mask, -1e9)\n",
    "        attn = f.softmax(scores, dim=-1)\n",
    "        #print(\"attn\", attn.shape)\n",
    "        #attn = self.idaten(attn)\n",
    "        context = torch.bmm(attn, value)\n",
    "        #print(\"context\", context.shape)\n",
    "        return context,attn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, dim_inp, dim_out):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(dim_inp, dim_out) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.linear = nn.Linear(dim_out * num_heads, dim_inp)\n",
    "        self.norm = nn.LayerNorm(dim_inp)\n",
    "        \n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        #s = [head(input_tensor, attention_mask) for head in self.heads]\n",
    "        s = []\n",
    "        s_atten = []\n",
    "        for head in self.heads:\n",
    "            res, att_out = head(input_tensor, attention_mask)\n",
    "            s.append(res)\n",
    "            s_atten.append(att_out)\n",
    "        att_map = torch.cat(s_atten, dim=0)\n",
    "        scores = torch.cat(s, dim=-1)\n",
    "        scores = self.linear(scores)\n",
    "        return self.norm(scores), att_map\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_inp, dim_out, attention_heads=4, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(attention_heads, dim_inp, dim_out)  # batch_size x sentence size x dim_inp\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(dim_inp, dim_out),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim_out, dim_inp),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(dim_inp)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        context, att_map = self.attention(input_tensor, attention_mask)\n",
    "        res = self.feed_forward(context)\n",
    "        return self.norm(res), att_map\n",
    "\n",
    "\n",
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, dim_inp, dim_out, attention_heads=4, ds_vocab=None):\n",
    "        super(BERT, self).__init__()\n",
    "        \n",
    "        self.ds_vocab = ds_vocab\n",
    "        \n",
    "        self.embedding = JointEmbedding(vocab_size, dim_inp, self.ds_vocab)\n",
    "        self.encoder = Encoder(dim_inp, dim_out, attention_heads)\n",
    "\n",
    "        self.token_prediction_layer = nn.Linear(dim_inp, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        self.classification_layer = nn.Linear(dim_inp*2, 2)\n",
    "        self.classification_layer_uni_gru = nn.Linear(dim_inp*2, 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.gru = nn.GRU(32,\n",
    "                          dim_inp,\n",
    "                          num_layers=1,\n",
    "                          batch_first=True,\n",
    "                          dropout=.2,\n",
    "                          bidirectional=True)\n",
    "        \n",
    "        self.gru_uni = nn.GRU(32,\n",
    "                          dim_inp,\n",
    "                          num_layers=1,\n",
    "                          batch_first=True,\n",
    "                          dropout=.2,\n",
    "                          bidirectional=False)\n",
    "        \n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        embedded = self.embedding(input_tensor)\n",
    "        encoded, att_map = self.encoder(embedded, attention_mask)\n",
    "        token_predictions = self.token_prediction_layer(encoded)\n",
    "        \n",
    "        all_toks_reps = self.gru(encoded)[0]\n",
    "        #all_toks_reps = self.gru_uni(encoded)[0]\n",
    "        \n",
    "        linear_axuliary_output = nn.ReLU()((all_toks_reps[:, 0, :]))\n",
    "        #linear_axuliary_output = f.relu(all_toks_reps[:, -1, :])#(all_toks_reps)\n",
    "\n",
    "        return self.softmax(token_predictions), self.classification_layer_uni_gru(linear_axuliary_output),att_map, encoded\n",
    "    \n",
    "        #return self.softmax(token_predictions), self.classification_layer(linear_axuliary_output),att_map, encoded\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15236d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c5ae455",
   "metadata": {},
   "source": [
    "# Train the Transformer on MLM and TrajectoryOrder (TOO)  Objective "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "308ac80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gc\n",
    "device_cpu = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def percentage(batch_size: int, max_index: int, current_index: int):\n",
    "    \"\"\"Calculate epoch progress percentage\n",
    "\n",
    "    Args:\n",
    "        batch_size: batch size\n",
    "        max_index: max index in epoch\n",
    "        current_index: current index\n",
    "\n",
    "    Returns:\n",
    "        Passed percentage of dataset\n",
    "    \"\"\"\n",
    "    batched_max = max_index // batch_size\n",
    "    return round(current_index / batched_max * 100, 2)\n",
    "\n",
    "\n",
    "def nsp_accuracy(result: torch.Tensor, target: torch.Tensor):\n",
    "    \"\"\"Calculate NSP accuracy between two tensors\n",
    "\n",
    "    Args:\n",
    "        result: result calculated by model\n",
    "        target: real target\n",
    "\n",
    "    Returns:\n",
    "        NSP accuracy\n",
    "    \"\"\"\n",
    "    #result, target = torch.FloatTensor(result).reshape(-1,1), torch.FloatTensor(target).reshape(-1,1)\n",
    "    s = (result.argmax(1) == target.argmax(1)).sum()\n",
    "    return round(float(s / result.size(0)), 2)\n",
    "\n",
    "\n",
    "def token_accuracy(result: torch.Tensor, target: torch.Tensor, inverse_token_mask: torch.Tensor):\n",
    "    \"\"\"Calculate MLM accuracy between ONLY masked words\n",
    "\n",
    "    Args:\n",
    "        result: result calculated by model\n",
    "        target: real target\n",
    "        inverse_token_mask: well-known inverse token mask\n",
    "\n",
    "    Returns:\n",
    "        MLM accuracy, MLM Random accuracy\n",
    "    \"\"\"\n",
    "    #print(result.shape, \"result\", result)\n",
    "    #print(result.argmax(-1).shape, \"result.argmax(-1)\", result.argmax(-1))\n",
    "    #print(target.shape, \"target\", target)\n",
    "    #print(inverse_token_mask.shape, \"inverse_token_mask\", inverse_token_mask)\n",
    "\n",
    "    r = result.argmax(-1).masked_select(~inverse_token_mask)\n",
    "    t = target.masked_select(~inverse_token_mask)\n",
    "    s = (r == t).sum()\n",
    "    random_acc = ((1/result.size(2))*(~inverse_token_mask).sum())#/result.size(0)\n",
    "    return round(float(s / (~inverse_token_mask).sum()), 10),random_acc\n",
    "    #return round(float(s / (result.size(0) * result.size(1))), 10),random_acc\n",
    "\n",
    "\n",
    "class BertTrainer_bi:\n",
    "\n",
    "    def __init__(self,\n",
    "                 model: BERT,\n",
    "                 dataset_train_1: BertDatasetBuilder,\n",
    "                 dataset_train_2: BertDatasetBuilder,\n",
    "                 log_dir: Path,\n",
    "                 checkpoint_dir: Path = None,\n",
    "                 print_progress_every: int = 10,\n",
    "                 print_accuracy_every: int = 10,\n",
    "                 batch_size: int = 24,\n",
    "                 learning_rate: float = 0.005,\n",
    "                 epochs: int = 5,\n",
    "                 ):\n",
    "        self.model = model\n",
    "        self.dataset_1 = dataset_train_1\n",
    "        self.dataset_val_1 = ds_final_val_mlm\n",
    "        self.dataset_2 = dataset_train_2\n",
    "        self.dataset_val_2 = ds_final_val_swap\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.current_epoch = 0\n",
    "        self.save_weights_every = 5\n",
    "\n",
    "        self.loader_1 = DataLoader(self.dataset_1, batch_size=self.batch_size, shuffle=True)\n",
    "        #self.loader_val = DataLoader(self.dataset_val, batch_size=self.dataset_val.df.shape[0], shuffle=True)\n",
    "        self.loader_val_1 = DataLoader(self.dataset_val_1, batch_size=1024, shuffle=True)\n",
    "        \n",
    "        self.loader_2 = DataLoader(self.dataset_2, batch_size=self.batch_size, shuffle=True)\n",
    "        #self.loader_val = DataLoader(self.dataset_val, batch_size=self.dataset_val.df.shape[0], shuffle=True)\n",
    "        self.loader_val_2 = DataLoader(self.dataset_val_2, batch_size=1024, shuffle=True)\n",
    "        \n",
    "        self.writer = SummaryWriter(str(log_dir))\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "        self.criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        self.ml_criterion = nn.NLLLoss(ignore_index=0).to(device)\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.015)\n",
    "\n",
    "        self._splitter_size = 35\n",
    "\n",
    "        self._ds_len = len(self.dataset_1)\n",
    "        self._batched_len = self._ds_len // self.batch_size\n",
    "\n",
    "        self._print_every = print_progress_every\n",
    "        self._accuracy_every = print_accuracy_every\n",
    "\n",
    "    def print_summary(self):\n",
    "        ds_len = len(self.dataset_1)\n",
    "\n",
    "        print(\"Model Summary\\n\")\n",
    "        print('=' * self._splitter_size)\n",
    "        print(f\"Device: {device}\")\n",
    "        print(f\"Training dataset len: {ds_len}\")\n",
    "        print(f\"Max / Optimal sentence len: {self.dataset_1.optimal_sentence_length}\")\n",
    "        print(f\"Vocab size: {len(self.dataset_1.vocab)}\")\n",
    "        print(f\"Batch size: {self.batch_size}\")\n",
    "        print(f\"Batched dataset len: {self._batched_len}\")\n",
    "        print('=' * self._splitter_size)\n",
    "        print()\n",
    "\n",
    "    def __call__(self):\n",
    "        for self.current_epoch in range(self.current_epoch, self.epochs):\n",
    "            loss = self.train(self.current_epoch)\n",
    "            #save weights every n epchs and the last epochs\n",
    "            if self.current_epoch%self.save_weights_every==0 or self.current_epoch==self.epochs-1:\n",
    "                #save the vocabulary\n",
    "                \n",
    "                torch.save(self.dataset_1.vocab, self.checkpoint_dir.joinpath('vocab_obj_min20_drug_med_swap_med_diag_gru.pth'))\n",
    "                #save the model\n",
    "                print(\"SSSSSAVVVVVEEEE=  \", self.current_epoch)\n",
    "                self.save_checkpoint(self.current_epoch, step=-1, loss=loss)\n",
    "                \n",
    "    def relative_acc(self, all_actual_tok_list, all_preds_tok_list):\n",
    "        \n",
    "        conf_mat = confusion_matrix(all_actual_tok_list, all_preds_tok_list)\n",
    "        zero_row_mask = ~np.all(conf_mat == 0, axis=1)\n",
    "        conf_mat_without_zero_row = conf_mat[zero_row_mask]\n",
    "\n",
    "        diags = np.diagonal(conf_mat)[zero_row_mask]\n",
    "        class_sums = conf_mat_without_zero_row.sum(axis=1)\n",
    "        relative_acc = np.sum(np.divide(diags,class_sums))/conf_mat_without_zero_row.shape[0]\n",
    "        return relative_acc\n",
    "            \n",
    "    def evaluate_val_train(self,\n",
    "                           model_for_valid,\n",
    "                           index,\n",
    "                           avg_nsp_per_eporch_loss,\n",
    "                           avg_mlm_per_eporch_loss,\n",
    "                           avg_token_acc_train,\n",
    "                           avg_token_random_acc_train,\n",
    "                           relativ_acc_train,\n",
    "                          nsp_acc_train):\n",
    "        average_nsp_loss = 0\n",
    "        average_mlm_loss = 0\n",
    "        average_nsp_acc_val = 0\n",
    "        average_mlm_acc_val, average_mlm_random_acc_val = 0, 0\n",
    "        all_preds_tok_list = []\n",
    "        all_actual_tok_list = []\n",
    "        #all_preds_axul_list = []\n",
    "        #all_actual_axul_list = []\n",
    "        model_for_valid.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, value in enumerate(self.loader_val_1):\n",
    "                self.model.train(False)\n",
    "                inp, mask, inverse_token_mask, token_target, nsp_target = value\n",
    "                #inp, mask, inverse_token_mask, token_target, nsp_target = inp.to(device_cpu), mask.to(device_cpu), inverse_token_mask.to(device_cpu), token_target.to(device_cpu), nsp_target.to(device_cpu)\n",
    "                token, nsp, atten_map, encoded = model_for_valid(inp, mask)\n",
    "                tm = inverse_token_mask.unsqueeze(-1).expand_as(token)\n",
    "                token = token.masked_fill(tm, 0)\n",
    "                loss_token = self.ml_criterion(token.transpose(1, 2), token_target)  # 1D tensor as target is required\n",
    "                loss_nsp = self.criterion(nsp, nsp_target)\n",
    "                loss = loss_token + loss_nsp\n",
    "                average_nsp_loss += loss_nsp\n",
    "                average_mlm_loss += loss_token\n",
    "                #acc per epoch\n",
    "                token_acc, token_random_acc = token_accuracy(token, token_target, inverse_token_mask)\n",
    "                average_mlm_acc_val += token_acc\n",
    "                average_mlm_random_acc_val += token_random_acc\n",
    "                #relative acc per epoch\n",
    "                #pred_codes = np.array(token.transpose(1, 2).argmax(1).cpu())[(token.transpose(1, 2).argmax(1).cpu() !=0)]\n",
    "                #ALIIIII now\n",
    "                pred_codes = token.transpose(1, 2).argmax(1).masked_select(~inverse_token_mask)\n",
    "                all_preds_tok_list.extend(pred_codes.tolist())\n",
    "                actual_codes = token_target.masked_select(~inverse_token_mask)\n",
    "                all_actual_tok_list.extend(actual_codes.tolist())\n",
    "                average_nsp_acc_val+= nsp_accuracy(nsp, nsp_target)\n",
    "                #all_preds_axul_list.extend(nsp.tolist())\n",
    "                #all_actual_axul_list.extend(nsp_target.tolist())\n",
    "                #print(\"val\", len(all_preds_tok_list), len(all_actual_tok_list))\n",
    "                \n",
    "            average_nsp_loss = (average_nsp_loss/len(self.loader_val_1)).item()\n",
    "            average_mlm_loss = (average_mlm_loss/len(self.loader_val_1)).item()\n",
    "            average_mlm_acc_val = (average_mlm_acc_val/len(self.loader_val_1))\n",
    "            average_mlm_random_acc_val = (average_mlm_random_acc_val/len(self.loader_val_1)).item()\n",
    "            average_nsp_acc_val = (average_nsp_acc_val/len(self.loader_val_1))#.item()\n",
    "            ###\n",
    "            #average_nsp_acc_val = nsp_accuracy(all_preds_axul_list, all_actual_axul_list)\n",
    "            \n",
    "            #for i, value in enumerate(loader_val):\n",
    "                #inp, mask, inverse_token_mask, token_target, nsp_target = value\n",
    "                #token, nsp, atten_map, encoded = bert(inp, mask)\n",
    "                #tm = inverse_token_mask.unsqueeze(-1).expand_as(token)\n",
    "                #token = token.masked_fill(tm, 0)\n",
    "                #pred_codes = np.array(token.transpose(1, 2).argmax(1).cpu())[(token.transpose(1, 2).argmax(1).cpu() !=0)]\n",
    "                #all_preds_tok_list.extend(pred_codes.tolist())\n",
    "                #actual_codes = token_target.masked_select(~inverse_token_mask)#np.array(token_target.cpu())[(token_target.cpu() !=0)]\n",
    "                #all_actual_tok_list.extend(actual_codes.tolist())\n",
    "\n",
    "            ###\n",
    "            #print(\"all_actual_tok_list22\", len(all_actual_tok_list), len(all_preds_tok_list))\n",
    "            relativ_acc_val = self.relative_acc(all_actual_tok_list, all_preds_tok_list)\n",
    "            #print(\"valid loss tokken=\", average_mlm_loss,\n",
    "            #      \",valid loss NSP=\", average_nsp_loss)\n",
    "            \n",
    "            print(\"mlm_avg_loss_train = \", avg_mlm_per_eporch_loss,\"mlm_avg_loss_val\", average_mlm_loss)\n",
    "            print(\"nsp_avg_loss_train = \", avg_nsp_per_eporch_loss,\"nsp_avg_loss_val\", average_nsp_loss)\n",
    "            print(\"nsp_avg_acc_train = ,\",nsp_acc_train,  \"nsp_avg_acc_val = \", average_nsp_acc_val)\n",
    "            print(\"mlm_avg_acc_train = \", avg_token_acc_train,\"mlm_avg_acc_val\", average_mlm_acc_val)\n",
    "            print(\"mlm_avg_acc_random_train = \", avg_token_random_acc_train,\n",
    "                  \"mlm_avg_acc_random_val\", average_mlm_random_acc_val)\n",
    "            print(\"mlm_rel_acc_train = \", relativ_acc_train, \"mlm_rel_acc_valid = \", relativ_acc_val)\n",
    "            #print(\"xx\", avg_mlm_per_eporch_loss, avg_mlm_per_eporch_loss)\n",
    "            self.writer.add_scalars(\"MLM actual_loss\",\n",
    "                                   {\"avg_train_loss\":avg_mlm_per_eporch_loss,\n",
    "                                   \"avg_val_loss\":average_mlm_loss},\n",
    "                                   global_step=self.current_epoch)\n",
    "            self.writer.add_scalars(\"NSP actual_loss\",\n",
    "                                   {\"avg_train_loss\":avg_nsp_per_eporch_loss,\n",
    "                                   \"avg_val_loss\":average_nsp_loss},\n",
    "                                   global_step=self.current_epoch)\n",
    "            self.writer.add_scalars(\"Token actual_accuracy\",\n",
    "                                {\"Token_train_accuracy\":avg_token_acc_train,\n",
    "                                 \"Token_train_random_accuracy\":avg_token_random_acc_train,\n",
    "                                 \"Token_val_accuracy\":average_mlm_acc_val,\n",
    "                                 \"Token_val_random_accuracy\":average_mlm_random_acc_val,\n",
    "                                \"Token_val_relative_acc\": relativ_acc_val,\n",
    "                                \"Token_train_relative_acc\": relativ_acc_train,\n",
    "                                \"nsp_val_acc\":average_nsp_acc_val,\n",
    "                                \"nsp_train_acc\":nsp_acc_train},\n",
    "                                global_step=self.current_epoch)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #self.writer.add_scalar(\"nsp val_loss\", \n",
    "            #                       average_nsp_loss, \n",
    "            #                       global_step=self.current_epoch)\n",
    "            \n",
    "            \n",
    "\n",
    "    def train(self, epoch: int):\n",
    "        print(f\"Begin epoch {epoch}\")\n",
    "\n",
    "        prev = time.time()\n",
    "        average_nsp_loss = 0\n",
    "        average_mlm_loss = 0\n",
    "        avg_nsp_per_eporch_loss = 0\n",
    "        avg_mlm_per_eporch_loss = 0\n",
    "        avg_nsp_acc_train = 0\n",
    "        avg_token_acc_train, avg_token_random_acc_train = 0, 0\n",
    "        all_preds_tok_list = []\n",
    "        all_actual_tok_list = []\n",
    "        #all_preds_axul_list = []\n",
    "        #all_actual_axul_list = []\n",
    "        val_per_minbatch = 50\n",
    "        validation_counter = 0\n",
    "        print(len(self.loader_1), len(self.loader_2))\n",
    "        ratio_first_to_sec_task = int(len(self.loader_2)/len(self.loader_1))+1\n",
    "        print(\"ratio_first_to_sec_task = \", ratio_first_to_sec_task)\n",
    "        #for i, value in enumerate(zip(self.loader_1,self.loader_2)):\n",
    "        for i in range(len(self.loader_1)):\n",
    "            self.model.train()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            index = i + 1\n",
    "            validation_counter += 1\n",
    "            #value1, value2 = value\n",
    "            i, value1 = next(enumerate(self.loader_1))\n",
    "            #_, value2 = next(enumerate(self.loader_2))\n",
    "            #perform the first task\n",
    "            inp1, mask1, inverse_token_mask1, token_target1, nsp_target1 = value1 \n",
    "            #inp2, mask2, inverse_token_mask2, token_target2, nsp_target2 = value2\n",
    "            \n",
    "            \n",
    "            \n",
    "            token1, nsp, atten_map, encoded = self.model(inp1, mask1)\n",
    "\n",
    "            tm = inverse_token_mask1.unsqueeze(-1).expand_as(token1)\n",
    "            token = token1.masked_fill(tm, 0)\n",
    "\n",
    "            loss_token = self.ml_criterion(token1.transpose(1, 2), token_target1)  # 1D tensor as target is required\n",
    "            #loss_nsp = self.criterion(nsp, nsp_target1)\n",
    "\n",
    "            loss = loss_token \n",
    "            ##average_nsp_loss += loss_nsp\n",
    "            average_mlm_loss += loss_token\n",
    "\n",
    "            #loss.backward()\n",
    "            #self.optimizer.step()\n",
    "            #print(\"loss_token = \", loss_token)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #perfrom the second task\n",
    "            j = 0\n",
    "            while(j<ratio_first_to_sec_task):\n",
    "                \n",
    "                #value2 = next(self.loader_2, sentinel)\n",
    "                #if value2 is sentinel:\n",
    "                #    break\n",
    "                _, value2 = next(enumerate(self.loader_2))\n",
    "                inp2, mask2, inverse_token_mask2, token_target2, nsp_target2 = value2\n",
    "\n",
    "                #self.optimizer.zero_grad()\n",
    "\n",
    "                #print(\"inp.shape, mask.shape\", inp.shape, mask.shape)\n",
    "                token, nsp2, atten_map, encoded = self.model(inp2, mask2)\n",
    "\n",
    "                tm = inverse_token_mask2.unsqueeze(-1).expand_as(token)\n",
    "                token = token.masked_fill(tm, 0)\n",
    "\n",
    "                #loss_token = self.ml_criterion(token.transpose(1, 2), token_target2)  # 1D tensor as target is required\n",
    "                loss_nsp = self.criterion(nsp2, nsp_target2)\n",
    "                #print(\"nsp\", nsp)\n",
    "                #print(\"nsp_target2\", nsp_target2)\n",
    "                #print(\"loss_nsp\",loss_nsp)\n",
    "                between_loss_ratio = round(loss_token.item()/(loss_nsp.item()*2),1)\n",
    "                \n",
    "\n",
    "                loss +=  (loss_nsp*between_loss_ratio)\n",
    "                \n",
    "                \n",
    "                \n",
    "                average_nsp_loss += loss_nsp \n",
    "                #average_mlm_loss += loss_token\n",
    "                #print(\"loss_nsp = \", loss_nsp)\n",
    "\n",
    "\n",
    "                #loss.backward()\n",
    "                #self.optimizer.step()\n",
    "                j+=1\n",
    "\n",
    "\n",
    "                #loss.detach()\n",
    "                #end second task\n",
    "\n",
    "        \n",
    "\n",
    "            #apply gradient on losses\n",
    "            #print(\"loss total = \", loss)\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            if index % self._print_every == 0:\n",
    "                print(\"between_loss_ratio\", between_loss_ratio)\n",
    "                \n",
    "                elapsed = time.gmtime(time.time() - prev)\n",
    "                s = self.training_summary(elapsed, index, average_nsp_loss, average_mlm_loss)\n",
    "\n",
    "                if index % self._accuracy_every == 0:\n",
    "                    s += self.accuracy_summary(index, token1, nsp2, token_target1, nsp_target2, inverse_token_mask1)\n",
    "\n",
    "                print(s)\n",
    "                average_nsp_loss = 0\n",
    "                average_mlm_loss = 0\n",
    "                \n",
    "                \n",
    "            avg_nsp_per_eporch_loss += loss_nsp\n",
    "            avg_mlm_per_eporch_loss += loss_token\n",
    "            #acc per epoch\n",
    "            token_acc, token_random_acc = token_accuracy(token1, token_target1, inverse_token_mask1)\n",
    "            avg_token_acc_train += token_acc\n",
    "            avg_token_random_acc_train += token_random_acc \n",
    "            #relative acc per epoch\n",
    "            #pred_codes = np.array(token.transpose(1, 2).argmax(1).cpu())[(token.transpose(1, 2).argmax(1).cpu() !=0)]\n",
    "            #ALIIIII now\n",
    "            pred_codes = token1.transpose(1, 2).argmax(1).masked_select(~inverse_token_mask1)\n",
    "            all_preds_tok_list.extend(pred_codes.tolist())\n",
    "            actual_codes = token_target1.masked_select(~inverse_token_mask1)\n",
    "            all_actual_tok_list.extend(actual_codes.tolist())\n",
    "            avg_nsp_acc_train += nsp_accuracy(nsp2, nsp_target2)\n",
    "\n",
    "            \n",
    "            if validation_counter%len(self.loader_1) ==0:#print every epochs\n",
    "                avg_nsp_per_eporch_loss = (avg_nsp_per_eporch_loss/len(self.loader_1)).item()\n",
    "                avg_mlm_per_eporch_loss = (avg_mlm_per_eporch_loss/len(self.loader_1)).item()\n",
    "                avg_token_acc_train = (avg_token_acc_train/len(self.loader_1))\n",
    "                avg_token_random_acc_train = (avg_token_random_acc_train/len(self.loader_1)).item()\n",
    "                nsp_acc_train = (avg_nsp_acc_train/len(self.loader_1))#.item()\n",
    "                #print(np.sum(np.array(token.transpose(1, 2).argmax(1).cpu() !=0)))\n",
    "                #print(\"all_actual_tok_list\", len(all_actual_tok_list), len(all_preds_tok_list))\n",
    "                \n",
    "                relativ_acc_train = self.relative_acc(all_actual_tok_list, all_preds_tok_list)\n",
    "                self.evaluate_val_train(self.model,\n",
    "                                        index,\n",
    "                                        avg_nsp_per_eporch_loss,\n",
    "                                        avg_mlm_per_eporch_loss,\n",
    "                                        avg_token_acc_train,\n",
    "                                        avg_token_random_acc_train,\n",
    "                                        relativ_acc_train,\n",
    "                                       nsp_acc_train)\n",
    "                #self.writer.add_scalar(\"MLM actual_loss\", avg_mlm_per_eporch_loss, global_step=self.current_epoch)\n",
    "                #self.writer.add_scalar(\"NSP actual_loss\", avg_nsp_per_eporch_loss, global_step=self.current_epoch)\n",
    "                avg_nsp_per_eporch_loss = 0\n",
    "                avg_mlm_per_eporch_loss = 0\n",
    "                avg_token_acc_train, avg_token_random_acc_train = 0, 0\n",
    "                #calculate accuray on train and valid dataset\n",
    "                \n",
    "                \n",
    "            if i%100==0:\n",
    "                del inp1,  mask1,  inverse_token_mask1,  token_target1,  nsp_target1\n",
    "                del inp2,  mask2,  inverse_token_mask2,  token_target2,  nsp_target2\n",
    "                del loss_nsp\n",
    "                del loss_token\n",
    "                del loss\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        loss =2\n",
    "        return loss\n",
    "\n",
    "    def training_summary(self, elapsed, index, average_nsp_loss, average_mlm_loss):\n",
    "        passed = percentage(self.batch_size, self._ds_len, index)\n",
    "        global_step = self.current_epoch * len(self.loader_1) + index\n",
    "\n",
    "        print_nsp_loss = average_nsp_loss / self._print_every\n",
    "        print_mlm_loss = average_mlm_loss / self._print_every\n",
    "\n",
    "        s = f\"{time.strftime('%H:%M:%S', elapsed)}\"\n",
    "        s += f\" | Epoch {self.current_epoch + 1} | {index} / {self._batched_len} ({passed}%) | \" \\\n",
    "             f\"NSP loss {print_nsp_loss:6.2f} | MLM loss {print_mlm_loss:6.2f}\"\n",
    "\n",
    "        self.writer.add_scalar(\"NSP loss\", print_nsp_loss, global_step=global_step)\n",
    "        self.writer.add_scalar(\"MLM loss\", print_mlm_loss, global_step=global_step)\n",
    "        return s\n",
    "\n",
    "    def accuracy_summary(self, index, token, nsp, token_target, nsp_target, inverse_token_mask):\n",
    "        global_step = self.current_epoch * len(self.loader_1) + index\n",
    "        nsp_acc = nsp_accuracy(nsp, nsp_target)\n",
    "        token_acc, token_random_acc = token_accuracy(token, token_target, inverse_token_mask)\n",
    "        self.writer.add_scalar(\"NSP train accuracy\", nsp_acc, global_step=global_step)\n",
    "        \n",
    "        print(\"Token train accuracy = \", token_acc,\"Token random accuracy\", token_random_acc)\n",
    "        #self.writer.add_scalars(\"Token actual_accuracy\",\n",
    "        #                        {\"Token_train_accuracy\":token_acc,\n",
    "        #                         \"Token_train_random_accuracy\":token_random_acc},\n",
    "        #                        global_step=self.current_epoch)\n",
    "        \n",
    "        self.writer.add_scalar(\"Token train accuracy\", token_acc, global_step=global_step)\n",
    "\n",
    "        return f\" | NSP accuracy {nsp_acc} | Token accuracy {token_acc}\"\n",
    "\n",
    "    def save_checkpoint(self, epoch, step, loss):\n",
    "        if not self.checkpoint_dir:\n",
    "            return\n",
    "\n",
    "        prev = time.time()\n",
    "        name = f\"bert_1rand_cls_gru_epoch{epoch}_step{step}_{datetime.datetime.utcnow().timestamp():.0f}.pt\"\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, self.checkpoint_dir.joinpath(name))\n",
    "\n",
    "        print()\n",
    "        print('=' * self._splitter_size)\n",
    "        print(f\"Model saved as '{name}' for {time.time() - prev:.2f}s\")\n",
    "        print('=' * self._splitter_size)\n",
    "        print()\n",
    "\n",
    "    def load_checkpoint(self, path: Path):\n",
    "        print('=' * self._splitter_size)\n",
    "        print(f\"Restoring model {path}\")\n",
    "        checkpoint = torch.load(path)\n",
    "        self.current_epoch = checkpoint['epoch']\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(\"Model is restored.\")\n",
    "        print('=' * self._splitter_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6232f2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02c22fc2",
   "metadata": {},
   "source": [
    "## Build the datasets loader for the 2 tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "082e5749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal_token_len_per_row =  62\n",
      "Create vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 52832/52832 [00:01<00:00, 32732.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2000/2000 [00:03<00:00, 636.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of final dataframe is  (55754, 4)\n",
      "Create vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 52832/52832 [00:01<00:00, 33379.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2000/2000 [00:04<00:00, 495.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of final dataframe is  (59582, 4)\n",
      "Create vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 52832/52832 [00:01<00:00, 34713.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2000/2000 [00:04<00:00, 433.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of final dataframe is  (82469, 4)\n",
      "Create vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 52832/52832 [00:01<00:00, 32427.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2000/2000 [00:04<00:00, 404.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of final dataframe is  (90828, 4)\n"
     ]
    }
   ],
   "source": [
    "#main_task=\"swap\"+\"MLM\"\n",
    "#2 main task, MLM and swapping with 2 differnt dataset\n",
    "#main_task=\"MLM\", CCS:\"gcausal_swap_token\", CVS:\"gcaus_swap_visit\"\n",
    "#Random Code Swapping: \"random_swap_token\"\n",
    "#Random Visit Swapping: \"random_swap_visit\"\n",
    "\n",
    "\n",
    "ds_to = 2000#99000000#\n",
    "start_from_scratch = True\n",
    "\n",
    "if start_from_scratch:\n",
    "    training_vocab = None\n",
    "else:\n",
    "    training_vocab = torch.load('mlm3_bert_checkpoints_swap_med_diag_mlp/vocab_obj_min20_drug_med_swap_med_diag_mlp.pth')\n",
    "    vocab_size = len(training_vocab)\n",
    "#praparing datasets\n",
    "#preapre the mlm datset\n",
    "ds_mlm_daig_med_train = BertDatasetBuilder(BASE_DIR.joinpath('data/imdb.csv'), \n",
    "                                     input_data = X_train, \n",
    "                                     ds_from=0, \n",
    "                                     ds_to=ds_to,\n",
    "                                     vocabulary=training_vocab)\n",
    "\n",
    "\n",
    "\n",
    "ds_final_train_mlm = BertDatasetAggregate([ds_mlm_daig_med_train])\n",
    "\n",
    "\n",
    "#praparing validation dataset\n",
    "ds_mlm_daig_med_val = BertDatasetBuilder(BASE_DIR.joinpath('data/imdb.csv'),\n",
    "                                   input_data = X_val,\n",
    "                                   ds_from=0,\n",
    "                                   ds_to=ds_to,\n",
    "                                   vocabulary = ds_mlm_daig_med_train.vocab,\n",
    "                                   optimal_token_len_per_row = ds_mlm_daig_med_train.optimal_token_len_per_row)\n",
    "\n",
    "\n",
    "ds_final_val_mlm = BertDatasetAggregate([ds_mlm_daig_med_val])\n",
    "\n",
    "##prapare the swap dataset\n",
    "ds_swap_daig_med_train = BertDatasetBuilder(BASE_DIR.joinpath('data/imdb.csv'), \n",
    "                                     input_data = X_train, \n",
    "                                     ds_from=0, \n",
    "                                     ds_to=ds_to,\n",
    "                                     vocabulary = ds_mlm_daig_med_train.vocab,\n",
    "                                     optimal_token_len_per_row = ds_mlm_daig_med_train.optimal_token_len_per_row, \n",
    "                                     main_task=\"gcausal_swap\")\n",
    "\n",
    "\n",
    "\n",
    "ds_final_train_swap = BertDatasetAggregate([ds_swap_daig_med_train])\n",
    "\n",
    "\n",
    "#praparing validation dataset\n",
    "ds_swap_daig_med_val = BertDatasetBuilder(BASE_DIR.joinpath('data/imdb.csv'),\n",
    "                                   input_data = X_val,\n",
    "                                   ds_from=0,\n",
    "                                   ds_to=ds_to,\n",
    "                                   vocabulary = ds_mlm_daig_med_train.vocab,\n",
    "                                   optimal_token_len_per_row = ds_mlm_daig_med_train.optimal_token_len_per_row,\n",
    "                                   main_task=\"gcausal_swap\")\n",
    "\n",
    "\n",
    "ds_final_val_swap = BertDatasetAggregate([ds_swap_daig_med_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "680a2241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59582, 4) (55754, 4) 2338\n",
      "(90828, 4) (82469, 4) 2338\n"
     ]
    }
   ],
   "source": [
    "print(ds_final_val_mlm.df.shape, ds_final_train_mlm.df.shape, len(ds_final_val_mlm.vocab))\n",
    "print(ds_final_val_swap.df.shape, ds_final_train_swap.df.shape, len(ds_final_val_swap.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2664cf6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87b06716",
   "metadata": {},
   "source": [
    "# Train TOO-Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a804382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch\n",
    "from pathlib import Path\n",
    "BASE_DIR = Path(__file__).resolve()\n",
    "\n",
    "EMB_SIZE = 32\n",
    "HIDDEN_SIZE = 36\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 1024#512\n",
    "NUM_HEADS = 5\n",
    "\n",
    "\n",
    "CHECKPOINT_DIR = BASE_DIR.joinpath('mlm3_bert_checkpoints_swap_med_diag_gru')\n",
    "\n",
    "timestamp = datetime.datetime.utcnow().timestamp()\n",
    "LOG_DIR = BASE_DIR.joinpath(f'data/logs/bert_experiment_{timestamp}')\n",
    "\n",
    "#device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    training_vocab = None\n",
    "    \n",
    "    \n",
    "    if not(start_from_scratch):\n",
    "\n",
    "        \n",
    "        \n",
    "        path4 = \"mlm3_bert_checkpoints_swap_med_diag_gru/bert_epoch50_step-1_1688951092.pt\"\n",
    "        bert = BERT(vocab_size, EMB_SIZE, HIDDEN_SIZE, NUM_HEADS, ds_vocab=ds_final_train_mlm.vocab).to(device)\n",
    "        checkpoint = torch.load(path4, map_location=device)\n",
    "        current_epoch = checkpoint['epoch']\n",
    "        bert.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "    #print(\"Prepare dataset\")\n",
    "    #ds = BertDatasetBuilder(BASE_DIR.joinpath('data/imdb.csv'), input_data = X_train, ds_from=0, ds_to=99000000,\n",
    "    #                       vocabulary=training_vocab)\n",
    "    #ds_val = BertDatasetBuilder(BASE_DIR.joinpath('data/imdb.csv'),\n",
    "    #                         input_data = X_val,\n",
    "    #                         ds_from=0,\n",
    "    #                         ds_to=99000000,\n",
    "    #                            vocabulary = ds.vocab,\n",
    "    #                           optimal_token_len_per_row = ds.optimal_token_len_per_row)\n",
    "    \n",
    "    if start_from_scratch:\n",
    "        print(\"build the bert from scratch\")\n",
    "        bert = BERT(len(ds_final_train_mlm.vocab), EMB_SIZE, HIDDEN_SIZE, NUM_HEADS, ds_vocab=ds_final_train_mlm.vocab).to(device)\n",
    "\n",
    "    trainer = BertTrainer_bi(\n",
    "        model=bert,\n",
    "        dataset_train_1=ds_final_train_mlm,\n",
    "        dataset_train_2 = ds_final_train_swap,\n",
    "        log_dir=LOG_DIR,\n",
    "        checkpoint_dir=CHECKPOINT_DIR,\n",
    "        print_progress_every=20,\n",
    "        print_accuracy_every=200,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=0.00007,\n",
    "        epochs=EPOCHS\n",
    "    )\n",
    "\n",
    "    trainer.print_summary()\n",
    "    trainer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
